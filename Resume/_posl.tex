\posl{} permet de construire des solveurs suivant différentes étapes : 
\begin{enumerate}
\item  L'algorithme du solveur considéré est  exprimé via une décomposition  en modules de calcul. Ces modules sont implémentés à la   manière de {\it fonctions} séparées. Nous appelons \INTROom{} ces morceaux de calcul (figure \ref{subfig:modules}, \new{blocs bleus}). \new{En suite, il faut décider  quelles  sont les  types d'informations que l'on souhaite  recevoir des autres solveurs.  Ces informations sont encapsulées  dans des composants appelés  \opch{},  permettant  de transmettre  des  données entre solveurs (figure \ref{subfig:modules}, \new{bloc rouge})} \label{stages:module}

\item  Une {\it stratégie générique}  est codée  à travers  \posl{}, en utilisant les  opérateurs fournis par le langage appliqués  sur des \new{\ms{} \textit{abstraite} qui représentent les \textit{signatures} des} composants donnés lors l'étape \ref{stages:module}, pour créer \ass. Cette  stratégie   définie  non  seulement  les informations   échangées,  mais   détermine  également   l'exécution parallèle de  composants. Lors  de cette  étape, les  informations à partager sont  transmises via  les opérateurs  ad-hoc. On  peut voir cette étape comme la définition de la colonne vertébrale des solveurs.

\item  Les solveurs sont créés en instanciant \new{l'\as, par \oms{} et \opch} (figure \ref{subfig:as}). %puis en les assemblant 

\item \new{Les solveurs sont assemblés en utilisant les opérateurs de communication fournis par le langage, pour creér des strategies de communication. Cet entité final s'appelle \INTROsoset (figure \ref{subfig:comm}).}
\end{enumerate}

\begin{figure}
	\centering
	\subfloat[][Definition du \ms{} et l'\opchs{}]{
		\label{subfig:modules}
		\includegraphics[width=0.5\columnwidth]{modules_1}
	}\\
	\subfloat[][Definition de l'\as]{%
		\label{subfig:as}
		\includegraphics[width=0.9\columnwidth]{as_1}
	}\\
	\subfloat[][Definition de la strategie de communication]{
		\label{subfig:comm}
		\includegraphics[width=0.9\columnwidth]{conn_1}
	}
	\caption[]{Construire des solveurs parallèles avec \posl{}}
	\label{fig:posl}
\end{figure}%fr

Les sous-sections suivantes expliquent en détail chacune des étapes ci-dessus.

\subsection{Computation module}

Un \om{}  est la plus basique  et abstraite manière de  définir un composant de calcul. Il reçoit une entrée, exécute un algorithme interne et retourne une sortie. Dans ce papier, nous utilisons ce concept afin de décrire et  définir les composants de base d'un  solveur, qui seront assemblés par l'\as.  

Un \om{} représente un  morceau de l'algorithme  du solveur  qui est susceptible de changer au cours de  l'exécution.   Il  peut  être
dynamiquement remplacé  ou combiné avec d'autres  \oms, puisque les \oms{}  sont  également  des   informations  échangeables  entre  les
solveurs. De  cette manière,  le  solveur  peut changer/adapter  son comportement à  chaud, en combinant  ses \oms{} avec ceux  des autres solveurs. Ils sont  représentés par  des blocs  bleus dans  la figure~\ref{fig:posl}.

\begin{definition}\label{def:module} \textbf{(Computation Module)}
Un \om{} $\mathcal{O}m$ est une application définie par :
\begin{equation}
 \mathcal{C}m:\mathcal{I} \rightarrow \mathcal{O}
\end{equation}
\end{definition}

Dans (\ref{def:module}),  la nature de $\mathcal{D}$  et $\mathcal{I}$ dépend du type de \om{}.  Ils peuvent être soit une configuration, ou  un  ensemble de  configurations, ou un ensemble de  valeurs  de différents types de données, etc.

Soit une méta-heuristique de recherche locale, basée sur un algorithme bien connu, comme par exemple {\it Tabu Search}. Prenons l'exemple d'un  \om{} retournant  le voisinage  d'une configuration  donnée, pour une certaine métrique de voisinage. Cet \om{} peut être défini par la fonction suivante:

\begin{equation}
\mathcal{C}m:D_1\times D_2\times\dots\times D_n \rightarrow 2^{D_1\times D_2\times\dots\times D_n}
\end{equation}

où $D_i$  représente  la  définition  des  domaines  de  chacune  des variables de la configuration d'entrée.

\subsection{Communication module}

Les \opchs{} sont  les composants des solveurs en charge  de la réception des informations  communiquées entre  solveurs. Ils  peuvent interagir
avec les  \oms, en fonction de l'\as. Les \opchs{} jouent  le rôle de prise, permettant aux  solveurs de  se  brancher et  de recevoir  des informations. Il sont représentés en rouge dans la figure~\ref{subfig:modules}.

Un \opch{} peut recevoir deux types d'informations, provenant toujours d'un solveur tiers : des données et des \oms. En  ce qui concerne les \oms, leur  communication peut  se faire  via la  transmission d'identifiants permettant à chaque solveur de les instancier.

Pour faire  la distinction entre  les deux différents types  de \opchs, nous appelons \INTROdopch{} les \opchs{} responsables de la réception de données  et \INTROoopch{} ceux s'occupant de la réception et de l'instanciation de \oms.

\defname{Data communication module}{
Un \dopch{} $\mathcal{C}h$ est un composant produisant une application définie comme suit : 
\begin{equation}
\label{def:dopench}
\mathcal{C}h: I\times\left\{D\cup\left\{NULL\right\}\right\} \rightarrow D \cup \left\{NULL\right\}
\end{equation}
et retournant l'information  $\mathcal{I}$ provenant d'un solveur tiers,quelque soit l'entrée $\mathcal{U}$.
}

\begin{definition}\label{def:oopench} \textbf{(Object communication module)} 
Si nous notons $\mathbf{M}$ l'espace  de  tous  les \oms{} de la définition~\ref{def:module}, alors un \oopch{} $\mathcal{C}h$ est  un composant produisant un \om{}  venant d'un solveur tiers défini ainsi :
\begin{equation}
\mathcal{C}h:I\times\left\{\mathbf{M}\cup\left\{NULL\right\}\right\} \rightarrow O\cup\left\{NULL\right\}
\end{equation}
\end{definition}

Puisque les \opchs{}  reçoivent  des  informations  provenant  d'autres solveurs sans pour autant avoir de contrôle sur celles-ci, il est  nécessaire  de  définir   l'information  {\it  NULL},  signifiant l'absence  d'information.  La  figure~\ref{fig:ochperform}  montre  le mécanisme interne d'un  \opch{}. Si un \dopch{} reçoit une  information,  celle-ci   est  automatiquement  retournée  (figure~\ref{subfig:doch},  lignes bleues).  Si un  \oopch{} reçoit un \om{}, ce dernier est instancié et exécuté avec l'entrée de l'\opch, et  le résultat  est retourné  (figure \ref{subfig:ooch}, lignes bleues). Dans les deux  cas, si aucune information n'est reçue, l'\opch{}  retourne l'objet  {\it NULL}  (figure \ref{fig:ochperform}, lignes rouges).

\begin{figure}
	\centering
	\subfloat[][Data \opch]{
		\label{subfig:doch}
		\includegraphics[width=0.7\linewidth]{D_OCh} %[width=0.2\textwidth]{muta1}
	}
	\\%\hspace{0.05\textwidth}%
	\subfloat[][Object \opch]{%
		\label{subfig:ooch}
		\includegraphics[width=0.7\linewidth]{O_OCh}%[width=0.2\textwidth]{muta2}
	}
	\caption[]{Mécanisme interne du \opch}
	\label{fig:ochperform}
\end{figure}%fr

\subsection{Abstract solver}

L'\as{}  est le c\oe{}ur du solveur. Il joint les \oms{} et les \opchs{} de  manière cohérente, tout en leur restant  indépendant. Ceci signifie  qu'elle peut  changer ou  être modifiée  durant l'exécution, sans altérer  l'algorithme général  et en  respectant la  structure du solveur.  À  travers  l'\as, on peut décider également des informations à envoyer aux autres solveurs. \new{Chaque fois que nous combinons certains composants en utilisant des opérateurs \posl{}, nous créons un \INTROm.}

\begin{definition}
\label{def:cm}
Noté par la lettre $\mathcal{M}$, un {\bf \m} est:
\begin{enumerate}\renewcommand{\labelitemi}{\scriptsize$\blacksquare$}
\item un \om{}; ou
\item un \opch{}; ou
\item $\left[\mbox{OP } \mathcal{M}\right]$, \new{la composition d'un module $\mathcal{M}$ exécuté sequentielement, en retournant une sortie, en dépendant de la nature de l'opérateur unaire \emph{OP};} ou \label{subdef:seq_uni}
\item $\left[\mathcal{M}_1 \mbox{ OP } \mathcal{M}_2\right]$, \new{la composition de deux modules $\mathcal{M}_1$ et $\mathcal{M}_2$ exécuté séquentiellement, en retournant une sortie, en dépendant de la nature de l'opérateur binaire \emph{OP}.}\label{subdef:seq}
\item $\left[\mathcal{M}_1 \mbox{ OP } \mathcal{M}_2\right]$, \new{la composition de deux modules $\mathcal{M}_1$ et $\mathcal{M}_2$ exécuté, en retournant une sortie, en dépendant de la nature de l'opérateur binaire \emph{OP}. Ces deux opérateurs vont être exécutes en parallèle si et seulement si \emph{OP} support le parallélisme, ou il lance une exception en cas contraire.}\label{subdef:par}
\end{enumerate}
Nous notons par $\mathbf{M}$ l'espace des \ms, et nous appelons \cms{} à la composition de \ms{} présentés en \ref{subdef:seq_uni} \ref{subdef:seq}, et/ou \ref{subdef:par}.
\end{definition}

Pour illustrer la définition~\ref{def:cm}, la figure~\ref{fig:cm} montre graphiquement le concept de \cm.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{cm} %[width=0.2\textwidth]{muta1}
	\caption[]{Un \cm}
	\label{fig:cm}
\end{figure}%fr

Dans le  cas particulier  où un  des \cms{}  impliqués est  un \opch{}, chaque opérateur gère l'information {\it NULL} à sa manière.

Afin   de  grouper   des   modules,  nous   utiliserons  la   notation $\left|.\right|$ comme un groupe générique qui pourra être indifféremment interprété comme $[.]$ ou comme $\lbk . \rbk_p$.

L'opérateur suivant nous permet d'exécuter deux modules séquentiellement, l'un après l'autre.

\begin{definition}\label{def:seqexec} {\bf (Sequential Execution Operator)} 
Soient
\begin{inparaenum}[i)]
	\item $\mathcal{M}_1 : \mathcal{D}_1 \rightarrow \mathcal{I}_1$ et
	\item $\mathcal{M}_2 : \mathcal{D}_2 \rightarrow \mathcal{I}_2$.
\end{inparaenum}
deux \ms{} différents. \new{Alors l'opération  $\left|\mathcal{M}_1\poslop{\mapsto}\mathcal{M}_2\right|$ définit le \cm{} $\mathcal{M}_{seq}$ comme le résultat de l'exécution de $\mathcal{M}_1$ suivi de $\mathcal{M}_2$.}

\[
\mathcal{M}_{seq}:\mathcal{D}_1 \rightarrow \mathcal{I}_2
\]
\end{definition}


L'opérateur  présenté  dans  la  définition~\ref{def:seqexec}  est  un exemple d'opérateur ne  supportant pas une exécution  parallèle de ses \cms impliqués, puisque l'entrée du second \cm{} est la sortie du premier.

%\begin{tabular}{ll}
%	$\left[\mathcal{M}_1\poslop{\mapsto}\mathcal{M}_2\right]$ & \textcolor{darkgreen}{\bf OK!}\\
%	$\lbk\mathcal{M}_1\poslop{\mapsto}\mathcal{M}_2\rbk_p$ & \textcolor{red}{\bf Impossible}
%\end{tabular}

L'opérateur suivant  est utile  pour exécuter des  modules séquentiels créant des branchements de calcul selon une condition booléenne :

\begin{definition}\label{op:conditional}
{\bf (Conditional Execution Operator)} Soient 
\begin{inparaenum}[i)]
	\item $\mathcal{M}_1 : \mathcal{D} \rightarrow \mathcal{I}_1$ et 
	\item $\mathcal{M}_2 : \mathcal{D} \rightarrow \mathcal{I}_2$,
\end{inparaenum} 
deux \ms{} différents. \new{Alors l'opération $\left|\mathcal{M}_1\circled{?}_{<cond>}\mathcal{M}_2\right|$ définit le \cm{} $\mathcal{M}_{cond}$ le résultat de l'exécution en séquentiel de $\mathcal{M}_1$ si $<cond>$ es {\bf vrai} or $\mathcal{M}_2$, autrement}:

\[
\mathcal{M}_{cond}:\mathcal{D} \rightarrow \mathcal{I}_1 \cup \mathcal{I}_2 
\]
\end{definition}

Nous pouvons exécuter séquentiellement  des modules créant des boucles de calcul, en définissant les \cms{} avec un autre opérateur conditionnel:

\begin{definition}\label{op:cyclic}
{\bf (Cyclic Execution Operator)} Soit $\mathcal{M} : \mathcal{D} \rightarrow \mathcal{I}$ un \m, où $\mathcal{I} \subseteq \mathcal{D}$. \new{Alors, l'opération $\left|\circlearrowleft_{<cond>}\mathcal{M}\right|$ définit le \cm{} $\mathcal{M}_{cyc}$ en répétant séquentiellement l'exécution de $\mathcal{M}$ tant que $<cond>$ est {\bf vrai}:}

\[
\mathcal{M}_{cyc}:\mathcal{D} \rightarrow \mathcal{I} 
\]
\end{definition}

%Dans la figure~\ref{fig:ex1}, on  présente un exemple simple combinant des \ms utilisant les opérateurs de \posl{} introduis ci-dessus. L'algorithme~\ref{algo:ex1} montre le code correspondant. Cet exemple montre  trois \ms{} faisant  partie d'un \cm{} avec pour but de générer  une configuration initiale à partir de laquelle débutera un solveur de recherche locale. Nous avons ainsi :
%
%\begin{itemize}
%\item $\mathcal{M}_1$, générant une configuration aléatoire.
%\item {$\mathcal{M}_2$, sélectionnant une variable aléatoire  d'une  configuration donnée, et recopiant sa valeur dans une autre variable.}
%\item $\mathcal{M}_3$, stockant la meilleure configuration trouvée.
%\end{itemize}

%{Dans cet exemple, l'\module{} $\mathcal{M}_2$ est d'abord exécuté, suivi
%de $\mathcal{M}_3$  si tous les  éléments de la  configuration générée
%sont  différents} (\verb!<  cond  >!). {Sinon,  seul l'\module{}
%$\mathcal{M}_3$ est  exécuté. Cette  opération est répétée  un certain
%nombre de fois} (\verb!< stop_cond >!).
%
%\figalgosbs{
%	\centering
%% eric
%%	\includegraphics[width=\linewidth]{Ex1v2.eps}
%	\includegraphics[width=0.8\linewidth]{Ex1v2}
%	\caption{}\label{fig:ex1}
%}{
%%\caption{\af{} code for Figure \ref{fig:ex1}}
%\caption{Code \af{} de la figure \ref{fig:ex1}}
%\dontprintsemicolon
%\SetNoline
%
%\While{$<\text{stop\_cond}>$}{		
%	$\left[\mathcal{M}_1 \xmapsto[<cond>]{} \left\{\mathcal{M}_2;\left[\mathcal{M}_2 \longmapsto\mathcal{M}_3\right]\right\}\right]$\;			
%}
%\label{algo:ex1}
%}%fr

\posl{} offre la possibilité de faire muter les solveurs. En fonction de l'opération, un  ou plusieurs \m{} opérande(s) sera exécutée(s), mais seule la sortie de l'un d'entre  eux sera retournée par le \cm. Nous présentons  ces opérateurs  dans deux  définitions, groupant  ceux qui exécutent uniquement un opérande  de \m{}~(définition~\ref{op:rho} et \ref{op:or}) et ceux exécutant les deux opérandes~(définition~\ref{op:min}, \ref{op:max} et~\ref{op:race}).

\begin{definition}\label{op:rho}
{\bf Random Choice Operator} Soient
\begin{inparaenum}[i)]
	\item $\mathcal{M}_1 : \mathcal{D} \rightarrow \mathcal{I}_1$ et
	\item $\mathcal{M}_2 : \mathcal{D} \rightarrow \mathcal{I}_2$,
\end{inparaenum} 
deux \ms{} différents, et un numéro réel $\rho \in (0,1)$. \new{Alors, l'operation $\left|M_1\circled{$\rho$}\mathcal{M}_2\right|$ définit le \cm{} $\mathcal{M}_{rho}$ qui exécute $\mathcal{M}_1$ en suivant une probabilité $\rho$, ou en exécutant $\mathcal{M}_2$ en suivant une probabilité $(1-\rho)$:}

\[
\mathcal{M}_{rho}:\mathcal{D} \rightarrow \mathcal{I}_1 \cup \mathcal{I}_2 
\]
\end{definition}

\begin{definition}\label{op:or}
{\bf Not {\it NULL} Execution Operator} Soient
\begin{inparaenum}[i)]
	\item $\mathcal{M}_1 : \mathcal{D} \rightarrow \mathcal{I}_1$ et  
	\item $\mathcal{M}_2 : \mathcal{D} \rightarrow \mathcal{I}_2$,
\end{inparaenum} 
deux \ms{} différents. \new{Alors, l'operation $\left|\mathcal{M}_1\circled{$\vee$}\mathcal{M}_2\right|$ définit le \cm{} $\mathcal{M}_{non}$ qui exécute $\mathcal{M}_1$ et retourne une sortie si elle n'est pas{\it NULL}, ou exécute $\mathcal{M}_2$ et retourne une sortie autrement:}

\[
\mathcal{M}_{non}:\mathcal{D} \rightarrow \mathcal{I}_1 \cup \mathcal{I}_2 
\]
\end{definition}

La définition suivante  fait appelle aux notions  de {\it parallélisme coopératif} et de {\it parallélisme compétitif}. Nous disons qu'il y a  parallélisme  coopératif  quand  deux  unités  de  calcul  ou  plus s'exécutent simultanément, et que le résultat obtenu provient de la combinaison des  résultats calculés par  chaque unité de  calcul (voir définitions~\ref{op:min} et \ref{op:max}). À l'opposé, nous  considérons qu'il y  a parallélisme compétitif  lorsque le résultat obtenu  est une solution ne provenant  que d'un seul processus exécuté  en parallèle; en  général   le  premier   processus  à  terminer   (voir  définition \ref{op:race}). 

\begin{definition}\label{op:min}
{\bf Minimum Operator } Soient 
\begin{enumerate}%\begin{inparaenum}[i)]
	\item $\mathcal{M}_1 : \mathcal{D} \rightarrow \mathcal{I}_1$ et
	\item $\mathcal{M}_2 : \mathcal{D} \rightarrow \mathcal{I}_2$,
\end{enumerate}%\end{inparaenum} 
deux \ms{} différents. \new{Soient aussi $o_1$ et $o_2$ les sorties de $\mathcal{M}_1$ et $\mathcal{M}_2$, respectivement. Nous assumons qu'il existe un ordre total dans $I_1 \cup I_2$ où l'objet \emph{NULL} est la plus grand valeur. Alors, l'opération $\left|\mathcal{M}_1\circled{m}\mathcal{M}_2\right|$ définit le \cm{} $\mathcal{M}_{min}$ qui exécute $\mathcal{M}_1$ et $\mathcal{M}_2$, et retourne $\min\left\{o_1,o_2\right\}$:}

\[
\mathcal{M}_{min}:\mathcal{D} \rightarrow \mathcal{I}_1 \cup \mathcal{I}_2 
\]
\end{definition}

De la même manière nous définissons l'opérateur \textbf{Maximum}:

\begin{definition}\label{op:max}
{\bf Minimum Operator} Soient 
\begin{enumerate}%\begin{inparaenum}[i)]
	\item $\mathcal{M}_1 : \mathcal{D} \rightarrow \mathcal{I}_1$ et
	\item $\mathcal{M}_2 : \mathcal{D} \rightarrow \mathcal{I}_2$,
\end{enumerate}%\end{inparaenum} 
deux \ms{} différents. \new{Soient aussi $o_1$ et $o_2$ les sorties de $\mathcal{M}_1$ et $\mathcal{M}_2$, respectivement. Nous assumons qu'il existe un ordre total dans $I_1 \cup I_2$ où l'objet \emph{NULL} est la plus petite valeur. Alors, l'opération $\left|\mathcal{M}_1\circled{M}\mathcal{M}_2\right|$ définit le \cm{} $\mathcal{M}_{max}$ qui exécute $\mathcal{M}_1$ et $\mathcal{M}_2$, et retourne $\max\left\{o_1,o_2\right\}$:}

\[
\mathcal{M}_{max}:\mathcal{D} \rightarrow \mathcal{I}_1 \cup \mathcal{I}_2 
\]
\end{definition}

%\poslexample{The {\bf minimum operator} can be applied in the previews example to obtain an interesting behavior: When applying the acceptance criteria, suppose that we want to receive a configuration from another solver, to compare it with ours and select the one with the lowest cost. We can do that by applying the $\circled{m}$ operator to combine the \om{} $A$ with a \opch{} $C.M.$ (see Figure\ref{fig:min_example}):
%$$\left[I\poslop{\mapsto}\left[\circlearrowleft\left[\left[V\poslop{\mapsto}S\right]\poslop{\mapsto}\lbk A\poslop{m}C.M.\rbk_p\right]\right]\right]$$
%Notice that in this example, I can use the grouper $\lbk .\rbk_p$ since the {\bf minimum operator} supports parallelism.}
%
%\begin{figure}[h]
%	\centering	
%	\includegraphics[width=0.6\linewidth]{min.png}
%	\caption{Using {\bf minimum} operator}\label{fig:min_example}
%\end{figure}

\begin{definition}\label{op:race}
{\bf Race Operator} Soient
\begin{inparaenum}[i)]
	\item $\mathcal{M}_1 : \mathcal{D} \rightarrow \mathcal{I}_1$  et
	\item $\mathcal{M}_2 : \mathcal{D} \rightarrow \mathcal{I}_2$,
\end{inparaenum} deux \ms{} différents, où $\mathcal{D}_1 \subseteq \mathcal{D}_2$ et $\mathcal{I}_1 \subset \mathcal{I}_2$. \new{Alors, l'opération $\left|\mathcal{M}_1\circled{$\shortdownarrow$}\mathcal{M}_2\right|$ définit le \cm{} $\mathcal{M}_{race}$ qui exécute les deux \ms{} $\mathcal{M}_1$ et $\mathcal{M}_2$, et retourne la sortie du \m{} qui termine en premier:}

\[
\mathcal{M}_{race}:\mathcal{D} \rightarrow \mathcal{I}_1 \cup \mathcal{I}_2 
\]
\end{definition}

%\figalgosbs{\
%	\centering
%% eric
%%	\includegraphics[width=\linewidth]{Ex2v2.eps}
%	\includegraphics[width=0.8\linewidth]{Ex2v2}
%	\caption{}\label{fig:ex2}
%}{
%%\caption{\af{} code for Figure \ref{fig:ex2}}
%\caption{Code \af{} de la figure \ref{fig:ex2}}
%\dontprintsemicolon
%\SetNoline	
%$\mathcal{M}_1 \longmapsto \lbk\mathcal{M}_2\circled{$\times$}\lbk\mathcal{M}_3 \circled{$\shortdownarrow$} \mathcal{M}_4\rbk_p\rbk_p\longmapsto\mathcal{M}_5$\;			
%\label{algo:ex2}
%}%fr
%
%Nous  illustrons un  de  ces  trois opérateurs  dans  l'exemple de  la
%figure~\ref{fig:ex2}. Les modules rentrant en jeu sont :
%
%\begin{itemize}
%	\item $\mathcal{M}_1$, retournant une configuration
%	\item $\mathcal{M}_2$, calculant un paramètre de tolérance $\epsilon$
%	\item   $\mathcal{M}_3$  et   $\mathcal{M}_4$,  calculant   le
%          voisinage  $\mathcal{N}$ de  la  configuration provenant  de
%          $\mathcal{M}_1$.  Ces deux  \modules    calcule  $\mathcal{N}$ de  manière
%          différente, mais ils sont combinés de façon à ce que seul la
%          sortie du premier module terminant son calcul soit retournée.
%	\item  $\mathcal{M}_5$, sélectionnant  une configuration  dans
%          $\mathcal{N}$ améliorant  le coût  global avec  un tolérance
%          $\epsilon$,  à  la  manière   du  {\it  Threshold  Accepting
%            Method}~\cite{Boussaid2013}
%\end{itemize}
%
%Dans   l'algorithme~\ref{algo:ex2},   les    \ms   $\mathcal{M}_3$   et
%$\mathcal{M}_4$ sont  exécutés en  parallèle, mais  seule la  sortie du
%premier module à terminer son calcul sera retournée. 
%
%De  la   même  manière,  le   \cm{}  composé  de   $\mathcal{M}_3$  et
%$\mathcal{M}_4$ peut  être exécuté en parallèle  avec $\mathcal{M}_2$,
%parce qu'ils sont  indépendants l'un de l'autre et  que l'opérateur le
%permet. Il est important de souligner qu'ils peuvent être exécutés par
%l'opérateur $\circled{$\times$}$, puisqu'ils reçoivent la même entrée.

Les  opérateurs   introduits  par  les   définitions~\ref{op:rho}, \ref{op:or} et \ref{op:min} sont très  utiles en  terme de  partage d'informations entre   solveurs, mais également en terme de partage de comportements. Si un  des opérandes est un  \opch{} alors l'opérateur peut recevoir le \om{} d'un autre solveur, donnant la possibilité d'instancier ce module dans le solveur le réceptionnant. L'opérateur va soit instancier le module s'il  n'est pas {\it NULL} et l'exécuter, soit exécuter le module donné par le second opérande.

Maintenant, nous définissons les  opérateurs nous permettant d'envoyer de  l'information vers  d'autres  solveurs. Deux  types d'envois  sont possibles :
\begin{inparaenum}[i)]
	\item on exécute un \m{} et on envoie sa sortie,
	\item ou on envoie le \m{} lui-même.
\end{inparaenum}

\begin{definition}\label{op:osend}
{\bf Sending Data Operator} Soit $\mathcal{M} : \mathcal{D} \rightarrow \mathcal{I}$ un \m. \new{Alors, l'opération $\left|\senddataop{\mathcal{M}}\right|$ définit le \cm{} $\mathcal{M}_{sendD}$ qui exécute le \m{} $\mathcal{M}$ puis envoie la sortie vers un \opch:}

\[
\mathcal{M}_{sendD}:\mathcal{D} \rightarrow \mathcal{I}
\]
\end{definition}

\begin{definition}\label{op:msend}
{\bf Sending Module Operator} Soit $\mathcal{M} : \mathcal{D} \rightarrow \mathcal{I}$ un \m. \new{Alors, l'opération $\left|\sendmoduleop{\mathcal{M}}\right|$ définit le \cm{} $\mathcal{M}_{sendM}$ qui exécute le \m{} $\mathcal{M}$, puis envoie le \m{} lui même  vers un \opch:}

\[
\mathcal{M}_{sendM}:\mathcal{D} \rightarrow \mathcal{I}
\]
\end{definition}

Avec  les opérateurs  présentés jusqu'ici,  nous sommes  en mesure  de concevoir les \ass{} (ou algorithme) de résolution d'un problème de contraintes. Une fois un  tel \as{} définie, on  peut changer les composants (\oms{} et \opchs) auxquels elle fait appel, permettant  ainsi d'implémenter différents solveurs à partir du même \as{} mais composés de différents \ms, du moment que ces derniers respectent  la signature attendue, à savoir le types des entrées et sorties.

\new{Un \as{} est déclaré comme suit: après déclarer les noms de l'\mbox{\tet{\bf \as}} ({\it name}), la première ligne définit la liste des \oms{} abstraites ($\mathcal{L}^m$), la seconde ligne, la liste des \opchs{} abstraites ($\mathbf{M}$), puis l'algorithme du solver est définit comment le corps su solver (the root \cm{} $\mathbf{M}$), entre \mbox{\tet{\bf begin}} et \mbox{\tet{\bf end}}.}

\new{Un \as{} peut être déclaré par l'expression régulière suivante:}

\begin{center}
\tet{\bf abstract solver} {\it name} \tet{\bf computation}: $\mathcal{L}^m$ (\tet{\bf communication}: $\mathcal{L}^c$)? \tet{\bf begin} $\mathbf{M}$ \tet{\bf end}
\end{center}

Par exemple, l'algorithme~\ref{algo:as_example} montre l'\as{} correspondant a la figure~\ref{subfig:as}.

\begin{algorithm}[h]
\dontprintsemicolon
\SetNoline
\SetKwProg{myproc}{}{}{}
\myproc{\tet{\bf abstract solver} as\_01\;
\tet{\bf computation} : $I, V, S, A$ \; 
\tet{\bf connection}: $C.M.$}{
	\Begin{
		$I \poslop{\mapsto}$\\
		\While{$\left(\textbf{\Iter < } K_1\right)$}{
			$\left[V\poslop{\mapsto}S\poslop{\mapsto}\left[C.M.\poslop{m} \senddataop{A}\right]\right]$%
		}
	}
}
\caption{Pseudo-code \posl{} pour l'\as{} de la figure~\ref{subfig:as}}\label{algo:as_example}
\end{algorithm}

\subsection{Créer les solveurs}

\new{Maintenant on peut créer les solveurs en instanciant les \ms. Il est possible de faire ceci en spécifiant que un \mbox{\tet{\bf solver}} donné doit implémenter (en utilisant le mot clé \mbox{\tet{\bf implements}}) un \as{} donné, suivi par la liste de \omprefix{} puis \opchs{}. Ces \ms{} doivent correspondre avec les signatures exigé par l'\as.}

\begin{algorithm}[h]
\dontprintsemicolon
\SetNoline
\SetKwProg{myproc}{}{}{}
\tet{\bf solver} solver\_01 \tet{\bf implements} as\_01\;
\tet{\bf computation} : $I_{rand}, V_{1ch}, S_{best}, A_{AI}$ \; 
\tet{\bf connection}: $CM_{last}$\;
\caption{Une instanciation de l'\as{} présenté dans l'algorithme~\ref{algo:as_example}}\label{algo:solver_def}
\end{algorithm}

\subsection{Connecter les solveurs : créer le \soset}

\new{La dernière étape est connecter les solveurs entre eux. \posl{} fournit des outils pour créer des stratégies de communication très facilement. L'ensemble des solveurs connectés qui seront exécutés en parallèle pour résoudre un CSP s'appelle \INTROsoset{}.}

Les communications sont établies en respectant les règles suivantes :
\begin{enumerate}
\item  À chaque  fois qu'un  solveur  envoie une  information via  les opérateurs  $\llparenthesis .\rrparenthesis^{d}$  ou $\llparenthesis   .\rrparenthesis^{m}$, il créé une {\it prise mâle de communication} 
\item À  chaque fois qu'un  solveur contient  un \opch{}, il  créé une {\it prise femelle de communication} 
\item Les solveurs peuvent être connectés entre eux en reliant {\it prises mâles} et {\it femelles}.
\end{enumerate}

Avec l'opérateur~$(\cdot)$, nous  pouvons  avoir  accès aux  \oms{} envoyant une information et aux noms des \opchs d'un solveur. Par exemple : $Solver_1\cdot\mathcal{M}_1$ fournit  un accès à le \om{} $\mathcal{M}_1$ du $Solver_1$ si et seulement s'il est utilisé par l'opérateur  $\llparenthesis .\rrparenthesis^{o}$  (ou $\llparenthesis.\rrparenthesis^{m}$), et $Solver_2\cdot\mathcal{C}h_2$ fournit un accès au \opch{} $\mathcal{C}h_2$ de $Solver_2$.

Maintenant, nous définissons les opérateurs de communication que \posl{} fournit.

\begin{definition}\label{op_conn:1to1}
{\bf Connection One-to-One Operator} Soient
\begin{enumerate}
\item $\mathcal{J} = \left[\mathcal{S}_0\cdot \mathcal{M}_0, \mathcal{S}_1\cdot \mathcal{M}_1,\dots, \mathcal{S}_{N-1}\cdot \mathcal{M}_{N-1}\right]$ une liste de  {\it prises mâles}, et
\item $\mathcal{O} = \left[\mathcal{Z}_0\cdot \mathcal{CM}_0, \mathcal{Z}_1\cdot \mathcal{CM}_1,\dots, \mathcal{Z}_{N-1}\cdot \mathcal{CM}_{N-1}\right]$ une liste de {\it prises femelles}
\end{enumerate} Alors, l'opération
\[
\mathcal{J} \poslop{\rightarrow} \mathcal{O}
\]
\new{connecte chaque {\it prise mâles} $\mathcal{S}_i\cdot \mathcal{M}_i \in \mathcal{J}$ avec la correspondante {\it prise femelle} $\mathcal{Z}_i\cdot \mathcal{CM}_i \in \mathcal{O}$, $\forall\textbf{ }0 \leq i \leq N-1$ (voir figure~\ref{subfig:comm_simple}).}
\end{definition}

\begin{definition}\label{op_conn:1ton}
{\bf Connection One-to-N Operator} Soient 
\begin{enumerate} 
\item $\mathcal{J} = \left[\mathcal{S}_0\cdot \mathcal{M}_0, \mathcal{S}_1\cdot \mathcal{M}_1,\dots, \mathcal{S}_{N-1}\cdot \mathcal{M}_{N-1}\right]$ une liste de  {\it prises mâles}, et
\item $\mathcal{O} = \left[\mathcal{Z}_0\cdot \mathcal{CM}_0, \mathcal{Z}_1\cdot \mathcal{CM}_1,\dots, \mathcal{Z}_{M-1}\cdot \mathcal{CM}_{M-1}\right]$ une liste de {\it prises femelles}
\end{enumerate} Alors, l'opération
\[
\mathcal{J} \poslop{\rightsquigarrow} \mathcal{O}
\]
\new{connecte chaque {\it prise mâles} $\mathcal{S}_i\cdot \mathcal{M}_i \in \mathcal{J}$ avec chaque {\it prise femelle} $\mathcal{Z}_j\cdot \mathcal{CM}_j \in \mathcal{O}$, $\forall\textbf{ }0 \leq i \leq N-1$ et $0 \leq j \leq M-1$ (see Figure~\ref{subfig:comm_diff}).}
\end{definition}


\posl{} permet aussi de déclarer des solveurs non communicatifs pour les exécuter en parallèle, en déclarant seulement la liste des noms:
\[
\left[\mathcal{S}_0, \mathcal{S}_1, \dots, \mathcal{S}_{N-1}\right]
\]

\begin{figure}[h]
\centering
\subfloat[][Communication 1 à 1]{
	\label{subfig:comm_simple}
	\includegraphics[width=0.4\columnwidth]{comm_11.png}
}
\hspace{0.05\textwidth}%
\subfloat[][Communication 1 à N]{%
	\label{subfig:comm_diff}
	\includegraphics[width=0.4\columnwidth]{comm_1n.png}
}
\caption[]{Représentation graphique des opérateurs de communication}
\label{fig:comm}
\end{figure}