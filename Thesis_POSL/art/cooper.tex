The interaction between solvers exchanging information is called {\it solver cooperation}. %and it is very popular in this field due to their good results. 
Its main goal is to improve some kind of limitations or inefficiency imposed by the use of unique solver. In practice, each solver runs in a computation unit, i.e. thread or processor. The cooperation is performed through inter--process communication, by using different methods: \textit{signals}, asynchronous notifications between processes in order to notify an event occurrence; \textit{semaphore}, an abstract data type for controlling access, by multiple processes, to a common resource; \textit{shared memory}, a memory simultaneously accessible by multiple processes; \textit{message passing}, allowing multiple programs to communicate using messages; among others.

Many times a close collaboration between process is required, in order to achieve the solution. But the first inconvenient is the slowness of the communication process. Some work have achieved to identify what information is viable to share. One example is the work presented by \etal{Hamadi} in \cite{Hamadi2012}, where an idea to include low-level reasoning components in the SAT problems resolution is proposed, dynamically adjusting the size of shared clauses to reduce the possible blow up in communication. This approach allows to perform the clause-sharing, controlling the exchange between any pair of processes.

This is a very changeling field, that is way we can find a lot of interesting ideas in the literature to improve parallel solutions through solver cooperation techniques. 

\etal{Kishimoto} present in \cite{Kishimoto2009} a parallelization of an algorithm A$^*$ (Hash Distributed A$^*$) for \textit{optimal sequential planning} \cite{Schmegner2004}, exploiting distributed memory computers clusters, to extract significant speedups from the hardware. In classical planning solving, both the memory and the CPU requirements are main causes of performance bottlenecks, so parallel algorithms have the potential to provide required resources to solve changeling instances. 

In \cite{Pajot2003} Pajot and Monfroy present a paradigm that enables the user to properly separate strategies combining solver applications, from the way the search space is explored in solver cooperations. The cooperation must be supervised by the user, through {\it cooperation strategy language}, which defines the solver interactions during the search process.

{\sc Meta--S} is an implementation of a theoretical framework proposed in \cite{Frank2003} by \etal{Franc}, which allows to tackle constrained problems, through the cooperation of arbitrary domain--specific constraint solvers. Through its modular structure and its extensible strategy specification language, it also serves as a test--bed for generic and problem--specific \nobreak{(meta-)solving} strategies, which are employed to minimize the incurred cooperation overhead. Treating the employed solvers as black boxes, the meta--solver takes constraints from a global pool and propagates them to the individual solvers, which are in return requested to provide newly gained information (i.e., constraints) back to the meta--solver, through variable projections. The major advantage of this approach lies in the ability to integrate arbitrary, new or pre--existing constraint solvers, to form a system that is capable of solving complex mixed--domain constraint problems, at the price of increased cooperation overhead. This overhead can however be reduced through more intelligent and/or problem--specific cooperative solving strategies. 

%In \cite{Frank2003} have been presented an implementation of the meta-solver framework which coordinates the cooperative work of arbitrary pluggable constraint solvers. This approach intents to integrate arbitrary, new or pre--existing constraint solvers, to form a system capable of solving complex mixed--domain constraint problems. The existing increased cooperation overhead is reduced through problem-specific cooperative solving strategies.

%{\sc Hyperion} \cite{Brownlee2014} is an already mentioned framework for meta-- and hyper--heuristics built with the principle of interoperability, generality by providing generic templates for a variety of local search and evolutionary computation algorithms; and efficiency, allowing rapid prototyping with the possibility of reusing source code.

Arbab and Monfory propose in \cite{Arbab2000} a technique to guide the search by splitting the domain of variables. A \textit{master} process builds the network of variables and domain reduction functions, and sends this information to the \textit{worker} processes. Workers concentrate their efforts on only one sub-\csp{} and the master collects solutions. The main advantage is that by changing only the search agent, different kinds of search can be performed. The coordination process is managing using the {\sc Manifold} coordination language \cite{Arbab1995}.

A component-based constraint solver in parallel is proposed in \cite{Zoeteweij} by Zoeteweij and Arbab. In this work, a parallel solver coordinates autonomous instances of a sequential constraint solver, which is used as a software component. The component solvers achieve load balancing of tree search through a time-out mechanism. It is implemented a specific mode of solver cooperation that aims at reducing the turn-around time of constraint solving through parallelization of tree search. The main idea is to try to solve a \csp{} before a time-out. If it cannot find a solution, the algorithm defines a set of disjoint sub-problems to be distributed among a set of solvers running in parallel. The goal of the time-out mechanism is to provide an implicit load balancing: when a solver is idle, and there are no subproblems available, another solver produces new sub-problems when its time-out elapses.

%{\sc Manifold} is a strongly-typed, block-structured, event-driven language for managing events, dynamically changing interconnections among sets of independent, concurrent and cooperative processes. A {\sc Manifold} application consists of a number of processes running on a heterogeneous network. Processes in the same application may be written in different programming languages. {\sc Manifold} has been successfully used in a broad range of applications \cite{Arbab1995}.

\etal{Munera} present in \cite{Munera} a new paradigm that includes cooperation between processes, in order to improve the independent multi-walk approach. In that case, cooperative search methods add a communication mechanism to the independent walk strategy, to share or exchange information between solver instances during the search process. This proposed framework is oriented towards distributed architectures based on clusters of nodes, with the notion of {\it teams} running on nodes and controlling several search engines ({\it explorers}) running on cores. All teams are distributed and thus have limited inter--node communication. This tool provides diversification through communication between teams, extending the search to different regions of the search space. Intensification is ensured through communication between explorers, and it is achieved swarming to the most promising neighborhood found by explorers. %This framework is oriented towards distributed architectures based on clusters of nodes, where teams are mapped to nodes and explorers run on cores. 
This framework was developed using the {\it X10 programming language}, which is a novel language for parallel processing developed by IBM Research, giving more flexibility than traditional approaches, e.g. MPI communication package.

A similar approach is presented by \etal{Guo} in \cite{Guo2010}, exploring principles of diversification and intensification in portfolio--based parallel SAT solving. To study their trade--off, they define two roles for the computational units. Some of them classified as {\it masters} perform an original search strategy, ensuring diversification. The remaining units, classified as {\it slaves} are there to intensify their master's strategy. 
%There are some important questions to be answered:
%\begin{inparaenum}[i)]
%	\item what information should be given to a slave in order to intensify a given search effort?, 
%	\item how often, a subordinated unit has to receive such information? and 
%	\item the question of finding the number of subordinated units and their connections with the search efforts? 
%\end{inparaenum}
Results lead to an original intensification strategy which outperforms the best parallel SAT solver {\it ManySAT}, and solves some open SAT instances.

\etal{Hamadi} propose in \cite{Hamadi2011} the first {\it Deterministic Parallel DPLL} (a complete, backtracking-based search algorithm for deciding the satisfiability of propositional logic formulas in conjunctive normal form) engine. The experimental results show that their approach preserves the performance of the parallel portfolio approach while ensuring full reproducibility of the results. Parallel exploration of the search space, defines a controlled environment based on a total ordering of solvers interactions through synchronization barriers. The frequency of exchanges (conflict-clauses) influences considerably the performance of the solver. The paper explores the trade off between frequent synchronizing which allows the fast integration of foreign conflict--clauses at the cost of more synchronizing steps, and infrequent synchronizing at the cost of delayed foreign conflict-clauses integration.

Considering the problem of parallelizing restarted backtrack search (the problem of finding the right time to to restart the search after some fails), \etal{Cire} have developed in \cite{Cire2011} a simple technique for parallelizing restarted search deterministically. They demonstrate experimentally that they can achieve near--linear speed--ups in practice, when the number of processors is constant and the number of restarts grows to infinity. The proposed technique is the following: each parallel search process has its own local copy of a scheduling class which assigns restarts and their respective fail--limits to processors. This scheduling class computes the next {\it Luby} restart fail--limit and adds it to the processor that has the lowest number of accumulated fails so far, following an {\it earliest--start--time--first strategy}. Like this, the schedule is filled and each process can infer which is the next fail--limit that it needs to run based on the processor it is running on -- without communication. Overhead is negligible in practice since the scheduling itself runs extremely fast compared to CP search, and communication is limited by informing other processes when a solution has been found.