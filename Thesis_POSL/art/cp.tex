\csps{} find a lot of "real-world" applications in the industry. In practice, these problems are tackled through different techniques. One of the most popular is \textit{constraint programming}, a combination of three main ingredients: \begin{inparaenum}[i)] \item a declarative model of the problem, \item constraint reasoning techniques like \textit{filtering} and \textit{propagation}, and \item search techniques. \end{inparaenum} This field is a famous research topic developed by the field of artificial intelligence in the middle of the 70's, and a programming paradigm since the end of the 80's.

Modeling a constrained problem, to be solved using \cp{} techniques, means properly choosing variables and their domains, and a right and efficient representation of the constraints set, aiming to declare as explicitly as possible, the solutions space. On the right election of the problem's model depends not only finding the solution, but also doing it in a fast and efficient way.

For modeling \csps{}, two tools can be highlighted. {\sc MiniZinc} is a simple but expressive constraint programming modeling language which is suitable for modeling problems for a range of solvers. It is the most used language for codding \csps{} \cite{Nethercote}. {\sc XCSP} is a readable, concise and structured XML-like language for coding \csps. This format allows to represent constraints defined either extensionally or intensionally. %Is not more used than {\sc MiniZinc} but although 
It was mainly used as the standard in the {\it International Constraint Solver Competition} (ended in 2009), and the {\it ICSC} dataset is for sure the biggest dataset of \csps{} instances existing today. \new{{\sc XCSP3}\footnote{XCSP3 website: \href{www.xcsp.org}{www.xcsp.org}} is the last upgrade of this format and it is available on Internet. Compact and easy to parse, {\sc XCSP3} is able to capture the structure of the problem models, and identifying syntactic and semantic groups of constraints. It introduces a number of features that make it able to enclose practically all constraints that can be found in major constraint solvers \cite{Boussemart2016}}. 

Constraint reasoning techniques are filtering algorithms applied for each constraint to prune provably infeasible values from the domain of the involved variables. This process is called \textit{constraint propagation}, and they are methods used to modify a \CSP{} in order to reduce its variables domains, and turning the problem into one that is equivalent, but \new{with a smaller search space, hence} usually easier to solve \cite{ChristianBessiere2006}. The main goal is to choose one (or some) constraint(s) to enforce certain consistency levels in the constraints. Achieving global consistency is desirable, because only using brute force algorithms one can reach a solution, but this is computationally very costly and intractable. %extremely hard to obtain. 
For that reason, some other consistency levels, easier to achieve, have been defined, like for example, \textit{arc consistency} and \textit{bound consistency}, which means trying to find values in the variables domain which make constraint unsatisfiable, in order to remove them from the domain. The applied procedure to reduce the variable domains is called \textit{reduction function}, and it is applied until a new, "smaller" and easier to solve problem is obtained, and it can not be further reduced: a \textit{fixed point}. Local consistency restrictions on the filtering algorithms are necessary to ensure not loosing solution during the propagation process.

%Before presenting these two consistency levels mentioned before, is imperative to define some notations. 
We said that a variable $x \in c$, if it is involved into the constraint $c$. Let the set $Var(c) = \{x_1\dots x_k\}$ the set of variables  involved into a constraint $c$ be, denoted by $Var(c)$. Then, a constraint $c$ is called \textit{arc consistent} if for all $x_i \in Var(c)$ with $1\leq i\leq k$, and for all $v_j \in D_j$ with $1\leq j\leq \left\|D_j\right\|$:
\[
\exists (v_1, \dots, v_{i-1}, v_{i+1},\dots, v_k) \in D_1\times\dots\times D_{i-1}\times D_{i+1}\times\dots\times D_k
\]
such that $c(v_1, \dots, v_k)$ is fulfilled. In other words, $c$ is arc consistent if for each value of each variable, there exist values for the other variables fulfilling $c$. In that case, we said that each value in the domain of $x_i$ has a \textit{support} in the domain of the other variables.

We denote by $Bnd(D_i) = \left\{\min(D_i), \max(D_i)\right\}$ the bounds of the domain $D_i$. Then, a constraint is \textit{bound consistent} if for all $x_i \in Var(c)$ with $1\leq i\leq k$, and for all $v_j \in Bnd(D_j)$ with $1\leq j\leq 2$:
\[
\exists (v_1, \dots, v_{i-1}, v_{i+1},\dots, v_k) \in Bnd\left(D_1\right)\times\dots\times Bnd\left(D_{i-1}\right)\times Bnd\left(D_{i+1}\right)\times\dots\times Bnd\left(D_k\right)
\]
such that $c(v_1, \dots, v_k)$ is fulfilled. It means that each bound (min/max) in the domain of $x_i$ has a support in the bounds (min/max) of the other variables. As we can notice that arc consistency is a stronger property, but heavier to enforce.

Apt and Monfroy have proposed in \cite{Apt} and \cite{Monfroy}, respectively, a formalization of constraint propagation through \textit{chaotic iterations}, which is a technique that comes from numerical analysis to compute limits of iterations of finite sets of functions, and adapted for computer science needs for naturally explain constraint propagation \cite{Chazan1969, Cousot1977}. Another approach is presented by Monfroy in \cite{Monfroy2000}, a coordination-based chaotic iteration algorithm for constraint propagation, which is a scalable, flexible and generic framework for constraint propagation using coordination languages, not requiring special modeling of \csps. Zoeteweij provides an implementation of this algorithm in {\sc DICE} (Distributed Constraint Environment) \cite{Zoeteweij2003} using the {\sc Manifold} coordination language. Coordination services implement existing protocols for constraint propagation, termination detection and splitting of \csps. {\sc DICE} combines these protocols with support for parallel search and the grouping of closely related components into cooperating solvers.

Another implementation of constraint propagation is proposed by \etal{Granvilliers} in \cite{Granvilliers2001}, using composition of reductions. It is a general algorithmic approach to tackle strategies that can be dynamically tuned with respect to the current state of constraint propagation, using composition operators. A composition operator models a sub--sequence of an iteration, in which the ordering of application of reduction functions is described by means of combinators for sequential, parallel or fixed--point computation, integrating smoothly the strategies to the model. This general framework provides a good level of abstraction for designing an object-oriented architecture of constraint propagation. Composition can be handled by the {\it Composite Design Pattern} \cite{DP_Composite}, supporting inheritance between elementary and compound reduction functions. The propagation mechanism uses the {\it Observer (Listener) Design Pattern} \cite{DP_Observer}, that makes the connection between domain modifications and re--invocation of reduction functions (event-based relations between objects); and the generic algorithm has been implemented using the {\it Strategy Design Pattern} \cite{DP_Strategy}, that allows to parametrize parts of algorithms.

A propagation engine prototype with a \textit{Domain Specific Language} (DSL) was implemented by \etal{Prud'homme} in \cite{Prudhomme2013}. It is a solver--independent language able to configure constraint propagations at the modeling stage. The main contributions are a DSL to ease configure constraint propagation engines, and the exploitation of the basic properties of DSL in order to ensure both completeness and correctness of the produced propagation engine. %, like:	
%\begin{inparaenum}[i)]%\begin{itemize}
%	\item {\it Solver independent description}: The DSL does not rely on specific solver requirements (but assuming that solvers provide full access to variable and propagator properties), 
%	\item {\it Expressivity}: The DSL covers commonly used data structures and characteristics, 
%	\item {\it Extensibility}: New attributes can be introduced to make group definition more concise. New collections and iterators can provide new propagation schemes, 
%	\item {\it Unique propagation}: The top-bottom left-right evaluation of the DSL ensures that each arc is only represented once in the propagation engine.
%\end{inparaenum}%\end{itemize}
Some characteristics are required to fully benefit from the DSL. Due to their positive impact on efficiency, modern constraint solvers already implement these techniques:
\begin{inparaenum}[i)] %\begin{itemize}
	\item Propagators are discriminated thanks to their priority (deciding which propagator to run next): lighter propagators (in the complexity sense) are executed before heavier ones.
	\item A controller propagator is attached to each group of propagators.
	\item Open access to variable and propagator properties: for instance, variable cardinality, propagator arity or propagator priority.
\end{inparaenum}%\end{itemize}
To be more flexible and more accurate, they assume that all arcs from the current \textit{CSP}, are explicitly accessible. This is achieved by explicitly representing all of them and associating them with {\it watched literals} \cite{Gent2006} (controlling the behavior of variable--value pairs to trigger propagation) or {\it advisors} \cite{Lagerkvist2007} (a method for supporting incremental propagation in propagator--centered setting). %{\it Advisors} in \cite{Lagerkvist2007} are used to modify propagator state and to decide whether a propagator must be propagated or "scheduled". 

Most of the times, we can not solve \csps{} only applying constraint propagation techniques. It is necessary to combine them with search algorithms. The complete search process consists in testing all possible configurations in an ordered way. Each time a partial evaluation is executed (evaluating just a set of variables), new constraints are posted, meaning that the propagation process can be relaunched. The simplest approach is using a backtracking search. It can be seen as performing a depth--first traversal of a search tree. This search tree is generated as the search progresses and represents alternative choices that may have to be examined in order to find a solution. Constraints are used to check whether a node may possibly lead to a solution of the \csp{} and to prune subtrees containing no solutions. A node in the search
tree is a \textit{dead-end} if it does not lead to a solution. Differences between searches lies in the selection criteria of the order of variables to be evaluated, and the order of the values to be assigned to variables. A \textit{static} search strategy is based on selecting the variable with me minimum index, to be evaluated first with the minimum value of its domain. Using this search, the \textit{tree structure} of the search space does not change, but is good for testing propagators. A \textit{dynamic} search strategy is based on selecting the variable with me minimum domain element, to be evaluated first with the minimum value of its domain. In this search strategy, propagators affect the variable selection order. A classical search strategy is based on selecting the variable with me minimum domain size, to be evaluated first with any values of its domain. Based on the \textit{first fail} principle which tells "\textit{Focus first on the variable that is more likely to cause a fail}",  this strategy works pretty well in many cases because by branching early on variables with a few value, the search tree becomes smaller.

In the field of \cp{} we can find a lot of solvers, able to solve constrained problems using these techniques. As examples, we can cite {\sc Cplex}\footnote{CPLEX Optimizer, available at: \href{http://www.ilog.com/products/cplex/}{http://www.ilog.com/products/cplex/}}, \textit{OR-tools}\footnote{Google Optimization Tools, available at: \href{https://developers.google.com/optimization/}{https://developers.google.com/optimization/}}, {\sc Gecode} and \choco. {\sc Cplex} is an analytical decision support toolkit for rapid development and deployment of optimization models using mathematical and constraint programming, to solve very large, real-world optimization problems. {\sc Gecode} is an efficient open source environment for developing constraint-based system and applications, that provides a modular and extensible constraint solver \cite{Gecode}, written in C++ (winner of all gold medals in the \textit{MiniZinc Challenge} from 2008 to 2012). During the formation phase of this PhD, we had the opportunity to perform some pedagogical experiments using two other important and recognized solvers: \textit{OR-tools} and \choco. The \textit{OR-tools} is an open source, portable and documented software suite for combinatorial optimization. It contains an efficient  constraint programming solver, used internally at Google, where speed and memory consumption are critical.

\choco{} is a free and open-source tool written in java, to describe hard combinatorial problems in the form of \csps{} and solving them using \CP{} techniques. Mainly developed by people at \'Ecole des Mines de Nantes (France), is a solver with a nice history, wining some awards, including seven medals in four entries in the \textit{MiniZinc Challenge}. This solver uses multi-thread approach for the resolution, and provides a problem modeler able to manipulate a wide variety of variable types. This problem modeler accepts over 70 constraints, including all classical arithmetical constraints, the possibility of using boolean operations between constraints, table constraints, i.e. defining the sets of tuples that verify the intended relation for a set of variables and a large set of useful classical global constraints including the \textit{alldifferent} constraint, the global \textit{cardinality} constraint, the \textit{cumulative} constraint, among others. \choco{} also contains a {\sc MiniZinc} and \textit{XCSP} instance parser. 
\choco{} can either deal with satisfaction or optimization problems. The search can be parameterized using a set of predefined variable and value selection heuristics, and also the variable and/or value selectors can be parametrized \cite{Jussien2008, Prudhomme2016}.

Although \cp{} techniques have shown very good results solving constrained problems, the search space in practical instances becomes intractable for them. For that reason, these constrained problems are mostly tackled by {\it meta-heuristic methods} or hybrid approaches. %, like \textit{Monte Carlo Tree Search} methods, which combine precision (tree search) with randomness (meta-heuristic) showing good results in artificial intelligence for games \cite{Chaslot2008, Browne2012}.