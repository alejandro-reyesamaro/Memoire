The \textit{Hybridization} approach is the one which combines different approaches into the same solution strategy, and recently, it leads to very good results in the constraint satisfaction field. We can find hybridization in combining algorithms in the same branch of investigation. This is the case of {\it ParadisEO}, a framework to design parallel and distributed hybrid meta-heuristics showing very good results, including a broad range of reusable features to easily design evolutionary algorithms and local search methods \cite{Cahon2004}. But we can find hybridization also in combining very different techniques, like the work of El-Ghazali Talbi presented in \cite{El-Ghazali2013}, which is a taxonomy of hybrid optimization algorithms is presented in an attempt to provide a mechanism to allow qualitative comparison of hybrid optimization algorithms, combining meta-heuristics with other optimization algorithms from mathematical programming, machine learning and constraint programming.

However, maybe one of the most common hybridization in this field, is the combination of meta-heuristic methods and constraint programming techniques. Constraint programming algorithms are based on backtracking mechanisms. These algorithms, also called {\it complete method} usually explore the search space systematically, and thus guarantee to find a solution if one exists. Meta-heuristic methods may find a solution to a problem, but they can fail even if the problem is satisfiable, because of its local nature. They perform a probabilistic exploration of the search space, so they are not able to guarantee finding a solution. For that reason they are also know as {\it incomplete methods}. However, they are more efficient (with respect to response time) than complete methods. The challenge is trying to get the best of both of these methods: exploration of a neighborhood from meta-heuristics, and the power of propagation from constraint programming \cite{Jussien2002,Pesant1996,Shaw1998}.

Hooker J.N. presents in \cite{Hooker2012} some ideas to illustrate the common structure present in exact and heuristic methods, to encourage the exchange of algorithmic techniques between them. The goal of this approach is to design solution methods able to smoothly transform their strategies from exhaustive to non-exhaustive search as the problem becomes more complex. Following this direction, \etal{Monfroy} present in \cite{Monfroya,Monfroyb} a general hybridization framework, proposed to combine complete constraints resolution techniques with meta-heuristic optimization methods in order to reduce the problem through domain reduction functions, ensuring not loosing solutions. 

A popular way of hybridization is the \textit{portfolio approach}, which is a methodology exploiting the significant variety in performances of different algorithms and combining them in order to create a globally better solver. In \cite{Amadini}, \etal{Amadini} propose \texttt{xcsp2mzn}, a tool for converting problem instances from the XCSP format %\cite{Committee} 
to {\sc MiniZinc}; %that is a simple but expressive constraint programming modeling language which is suitable for modeling problems for a range of solvers. It is the most used language for codding \csps{} \cite{Nethercote}. 
%The second contribution of this work is the development of
and \texttt{mzn2feat}, a tool to extract static and dynamic features from the {\sc MiniZinc} representation, with the help of the {\sc Gecode} interpreter, allowing a better and more accurate selection of the solvers to use according to the instances to solve. %Some results are showed proposing that the performances that can be obtained using these features are competitive with state of the art on \csp{} portfolio techniques. 
Based also on the portfolio approach, \etal{Amadini} propose in \cite{Amadini2014} a \textit{time splitting} technique to solve optimization problems. Given a problem $P$ and a schedule $Sch = \left[(\Sigma_1, t_1),\dots,(\Sigma_n, t_n)\right]$ of $n$ solvers, the corresponding time-split solver is defined as a particular solver such that:  
\begin{inparaenum} %\begin{enumerate}[label=\alph*)]
\item runs solver $\Sigma_1$ on $P$ for a period of time $t_1$, 
\item then, for $i = 1,\dots, n-1$, runs solver $\Sigma_{i+1}$ on $P$ for a period of time $t_{i+1}$ exploiting or not the best solution found by  the previous solver $\Sigma_i$ during $t_i$ units of time.
\end{inparaenum}%\end{enumerate}
{\it Autonomous search} is a technique based on supervised or controlled learning. This system are another portfolio point of view presented by \etal{Hamadi} in \cite{WhatIsAuto}, which improves its performance while it solves problems, either modifying its internal components to take advantage of the opportunities in the search space, or choosing adequately the solver to use.

An interesting hybridization point of view, is the integration of operations research into constraint programming. \etal{Fontaine} use in \cite{Fontaine2014} a generalization of the optimization paradigm \textit{Lagrangian relaxation}, to relax the hard constraints into the objective function, and applying them into constraint-programming and local search models. It combines concepts of constraint violation (typically used in constraint programming and local search) and constraint satisfiability (typically used in mathematical programming). Hooker J.N. presents in \cite{Hooker2006} a detailed description of how operation research models like mixed integer linear programming (MILP) models (which can themselves be relaxed), Lagrangian relaxations, and dynamic programming models can be applied to constraint programming. 

% COMENTAR 
%Nowadays there exists some tools to face this kind of problems. We can cite {\sc Choco}, an open source java constraint programming library \cite{Jussien2008}; ; ; {\sc Adaptive Search}, a constraint-based local search methods \cite{Diaz}; among others.

%COMENTAR   
%There exist also tools for modeling \textit{CSP} problems. Some of them intent to be a standards in terms of problem modeling.  Codding the problems using one of these tools (or both), it gives us the advantage of solving them using many solvers that support those languages. Furthermore, developing our own solver, it is also interesting to use them because we can test and compare our results using a wide range of available problems. 

%\nocite{Choco, Comet, CometPascal, Gecode, XCSP, Features, Minizinc, X10}