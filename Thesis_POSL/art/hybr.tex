The \textit{Hybridization} approach is the one who combine different approaches into the same solution strategy, and recently, it leads to very good results in the constraint satisfaction field. For example, constraint propagation may find a solution to a problem, but they can fail even if the problem is satisfiable, because of its local nature. At each step, the value of a small number of variables are changed, with the overall aim of increasing the number of constraints satisfied by this assignment, and applying other techniques to avoid local solutions, for example adding a stochastic component to choose variables to affect. Integrations of global search (complete search) with local search have been developed, leading to hybrid algorithms. 


An interesting hybridization point of view, is the integration of operations research into constraint programming. \etal{Fontaine} use in \cite{Fontaine2014} a generalization of the optimization paradigm \textit{Lagrangian relaxation}, to relax the hard constraints into the objective function, and applying them into constraint-programming and local search models. It combine the concepts of constraint violation (typically used in constraint programming and local search) and constraint satisfiability (typically used in mathematical programming). Hooker J.N. presents in \cite{Hooker2006} a detailed description of how operation research models like mixed integer linear programming (MILP) models (which can themselves be relaxed), Lagrangian relaxations, and dynamic programming models can be applied to constraint programming. 

Hooker J.N. presents in \cite{Hooker2012} some ideas to illustrate the common structure present in exact and heuristic methods, to encourage the exchange of algorithmic techniques between them. The goal of this approach is to design solution methods ables to smoothly transform its strategy from exhaustive to non-exhaustive search as the problem becomes more complex.

In \cite{El-Ghazali2013} a taxonomy of hybrid optimization algorithms is presented in an attempt to provide a mechanism to allow qualitative comparison of hybrid optimization algorithms, combining meta-heuristics with other optimization algorithms from mathematical programming, machine learning and constraint programming.

Monfroy et al. present in \cite{Monfroya,Monfroyb} a general hybridization framework, proposed to combine complete constraints resolution techniques with meta-heuristic optimization methods in order to reduce the problem through domain reduction functions, ensuring not loosing solutions. Other interesting ideas are {\sc Templar}, a framework to generate algorithms changing predefined components using hyper-heuristics methods \cite{Swan2015}; and {\it ParadisEO}, a framework to design parallel and distributed hybrid meta-heuristics showing very good results \cite{Cahon2004}, including a broad range of reusable features to easily design evolutionary algorithms and local search methods.

Another technique has been developed, the called {\it autonomous search}, based on the supervised or controlled learning. This systems improve their functioning while they solve problems, either  modifying their internal components to take advantage of the opportunities in the search space, or to adequately chose the solver to use ({\it portfolio point of view}) \cite{WhatIsAuto}.

In \cite{Amadini2014} is proposed another portfolio-based technique, \textit{time splitting}, to solve optimization problems. Given a problem \textit{P} and a schedule $Sch = \left[(\Sigma_1, t_1),\dots,(\Sigma_n, t_n)\right]$ of \textit{n} solvers, the corresponding time-split solver is defined as a particular solver such that:  
\begin{enumerate}[label=\alph*)]
\item runs $\Sigma_1$ on \textit{P} for a period of time $t_1$, 
\item then, for $i = 1,\dots, n-1$, runs $\Sigma_{i+1}$ on \textit{P} for a period of time $t_{i+1}$ exploiting or not the best solution found by the previous solver $\Sigma_i$ $t_i$ units of time.
\end{enumerate}

% COMENTAR 
%Nowadays there exists some tools to face this kind of problems. We can cite {\sc Choco}, an open source java constraint programming library \cite{Jussien2008}; ; ; {\sc Adaptive Search}, a constraint-based local search methods \cite{Diaz}; among others.

% Citas estas cosas en la parte donde propongo mi modelado :D

%COMENTAR   
%There exist also tools for modeling \textit{CSP} problems. Some of them intent to be a standards in terms of problem modeling.  Codding the problems using one of these tools (or both), it gives us the advantage of solving them using many solvers that support those languages. Furthermore, developing our own solver, it is also interesting to use them because we can test and compare our results using a wide range of available problems. 

In \cite{Amadini} is proposed a tool (\texttt{xcsp2mzn}) for converting problem instances from the format  \cite{Committee} to {\sc MiniZinc} that is a simple but expressive constraint programming modeling language which is suitable for modeling problems for a range of solvers. It is the most used language for codding \csps{} \cite{Nethercote}. The second contribution of this work is the development of \texttt{mzn2feat} a tool to extract static and dynamic features from the {\sc MiniZinc} representation, with the help of the {\sc Gecode} interpreter, and allows a better and more accurate selection of the solvers to be used according to the instances to solve. Some results are showed proposing that the performances that can be obtained using these features are competitive with state of the art on \csp{} portfolio techniques. 

%\nocite{Choco, Comet, CometPascal, Gecode, XCSP, Features, Minizinc, X10}