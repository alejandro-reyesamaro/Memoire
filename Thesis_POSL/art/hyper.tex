\textit{Hyper-heuristics} are automated methodologies for selecting or generating meta-heuristics algorithms to solve hard computational problems \cite{Chakhlevitch2008}. This can be achieved with a learning mechanism that evaluates the quality of the algorithm solutions, in order to become general enough to solve new instances of a given problem. \textit{Hyper-heuristics} are related with the \textit{Algorithm Selection Problem}, so they establish a close relationship between a problem instance, the algorithm to solve it and its performance \cite{Ryser-welch}. Hyper-heuristic frameworks are also known as \textit{Algorithm-Portfolio}--based frameworks. Their goal is predicting the running time of algorithms using statistical regression. Then the fastest predicted algorithm is used to solved the problem until a suitable solution is found or a time-out is reached~\cite{Leyton-Brown2003}.

This approach have been followed for solving constrained problems. {\sc Hyperion}$^2$ \cite{Brownlee2014} is a Java framework for meta-- and hyper-- heuristics which allows the analysis of the trace taken by an algorithm and its constituent components through the search space. It promotes interoperability via component interfaces, allowing rapid prototyping of meta- and hyper-heuristics, with the potential of using the same source code in either case. It also provides generic templates for a variety of local search and evolutionary computation algorithms, making easier the construction of novel meta- and hyper-heuristics by hybridization (via interface interoperability) or extension (subtype polymorphism). {\sc Hyperion}$^2$ is faithful to "{\it only pay for what you use}", a design philosophy that attempts to ensure that generality doesn't necessarily imply inefficiency. \textit{hMod} is inspired by the previous frameworks, but using a new object-oriented architecture. It encodes the core of the hyper-heuristic in several modules, referred as algorithm containers. \textit{hMod} directs the programmer to define the heuristic using two separate XML files; one for the heuristic selection process and the other one for the acceptance criteria~\cite{Urra2013}.

\textit{Evolving evolutionary algorithms} are specialized hyper-heuristic methods which attempt to readjust an evolutionary algorithm to the problem needs. An evolutionary algorithm (EA) discover the rules and knowledge to find the best algorithm to solve a problem. In \cite{Diosan2009} \etal{Dio\c{s}an} use linear genetic programming and multi-expression genetic programming to optimize the EA solving unimodal mathematical functions and another EA to adjust the sequence of genetic and reproductive operators. A solution consists of a new evolutionary algorithm capable of outperforming genetic algorithms when solving a specific class of unimodal test functions. An different but interesting point of view is presented in \cite{Samulowitz2013}, where \etal{Samulowitz} present \textit{Snappy}, a \textit{Simple Neighborhood-based Algorithm Portfolio} written in \textit{Python}. It is a very resent framework that aims to provide a tool able to improve its own performances through on-line learning. Instead of using the traditional off-line training step, a neighborhood search predicts the performance of the algorithms. It incorporates available knowledge coming from portfolio's runs, by considering the following ways incrementally: \begin{inparaenum}[1-] \item Every time a test instance is considered, it is added to the current set of training instances. \item After selecting an algorithm for a given test instance, the actual runtime information for the selected algorithm on this instance is added to the data set. It means that the difference between neighborhoods of different algorithms represent how often algorithms will be selected. \end{inparaenum} Other interesting idea is proposed by \etal{Swan} in {\sc Templar}, a framework to generate algorithms changing predefined components using hyper-heuristics methods~\cite{Swan2015}.