Despite advances previously presented, hard instances of many problems are still complicated to solve through these techniques. Thanks to \textit{parallel computing}, we have been capable of going one step further in solving \csps. Parallel computing is a way to solve problems using several computation resources at the same time. It is a powerful alternative to solve problems which would require too much time by using sequential algorithms \cite{Grama2003}. %That is why this field is in constant development and it is the topic where we put most of our effort. 

Since the late 2000's all processors in modern machines are multi-core. Massively parallel architectures, previously expensive and so far reserved for super--computers, become now a trend available to a broad public through hardware like the Xeon Phi or GPU cards. The power delivered by massively parallel architectures allow us to treat faster constrained problems \cite{Borkar2007}. However this architectural evolution is a non-sense if algorithms do not evolve at the same time: the development and the implementation of algorithms should take this into account and tackling problems with very different methods, changing the sequential reasoning of researchers in Computer Science \cite{Hill2008, Sanders2014}. 

In the literature on parallel constraint solving \cite{Gent}, two main limiting factors on performance are addressed: \begin{inparaenum}[1-] \item inter-process communication overheads (explained in details in Section~\ref{sec:cooperation}), and \item the \textit{Amdahl}'s law. \end{inparaenum} The Amdahl's law of parallel computing states that the \textit{speed-up} of a parallel algorithm is limited by the fraction of the program that must be executed sequentially. It means that adding more processors may not make the program run faster. It assumes that some percentage of the program or code cannot be parallelized ($T_{sequential}$), and states that the ratio called speed-up of $T_{sequential}$ over $1 - T_{Sequerntial} = T_{parallel}$ is bounded by $1\setminus T_{sequential}$ when the number of processors $P \rightarrow \infty$:

\begin{equation}\label{amdahl}
Speed-Up = \frac{T_{sequential}}{T_{parallel}} \leq \frac{1}{T_{sequential}}
\end{equation}

%\item parallelizing the search process,  
%\item parallel and distributed arc-consistency, 
%\item multi-agent and cooperative search and
%\item combined parallel search and parallel consistency.

Another issue, usually underestimated, is the codification. Writing efficient code for parallel machines is less trivial, as it usually deals with low-level APIs such as OpenMP and message-passing interfaces (MPI), among others. However, years of experience have shown that using those frameworks is difficult and error-prone. Usually many undesired behaviors (like deadlocks) make parallel software development very slow compared to sequential approaches. In that sense, Falcou proposes in \cite{Falcou2009} a programming model: \textit{parallel algorithmic skeletons} (along with a C++ implementation called {\sc Quaff}) to make parallel application development easier. This model is a high-order pattern to hide all low-level, architecture or framework dependent code from the user, and provides a decent level of organization. {\sc Quaff} is a skeleton-based parallel programming library, which has demonstrated its efficiency and expressiveness solving some application from computer vision. It relies on C++ template meta-programming to reduce the overhead traditionally associated with object-oriented implementations of such libraries allowing some code generation at compilation time. \etal{Cahon} also propose {\it ParadisEO}, a framework to design parallel and distributed hybrid meta-heuristics showing very good results, including a broad range of reusable features to easily design evolutionary algorithms and local search methods \cite{Cahon2004}.

%The contribution in terms of hardware has been crucial, achieving powerful technologies to perform large--scale calculations. 

The development of techniques and algorithms to solve problems in parallel focuses principally on three fundamental aspects: 
\begin{enumerate}%\begin{inparaenum}[i)]
    \item {\it Problem subdivision},
    \item {\it Search parallelization} and %{\it Scalability} and
    \item {\it Inter-process communication}.
\end{enumerate}%\end{inparaenum}
They all pursue the same objective: achieving good levels of \textit{scalability}. Scalability is the ability of a system to handle the increasing growth of workload. 
%A system which has improved over time its performance after adding work resources, and it is capable of doing it proportionally is called {\it scalable}. 
%The increase has not been only in terms of calculus resources, but also in the amount of sub-problems coming from the sub-division of the original problem. The more we can divide a problem into smaller sub-problems, the faster we can solve it \cite{Hill}. 
\textit{Adaptive Search} is a good example of local search method that can scale up to a larger number of cores, e.g., a few hundreds or even thousands of cores \cite{Diaz}. For this algorithm, an implementation of a cooperative multi-walks strategy has been published by \etal{Munera} in \cite{Munera}. In this framework processes are grouped in teams to achieve search intensification, which cooperate with others teams through a head node (process) to achieve search diversification. Using an adaptation of this method, authors propose a parallel solution strategy able to solve hard instances of \textit{Stable Marriage with Incomplete List and Ties Problem} quickly. This technique has been combined in \cite{Munera2016} with an \textit{Extremal Optimization} procedure: a nature-inspired general-purpose meta-heuristic \cite{Boettcher2000}.
	
The issue of subdividing a given problem in some smaller sub-problems is sometimes not easy to address. Even when we can do it, the time needed by each process to solve its own part of the problem is rarely balanced. For that reason it is imperative to apply some complementary techniques to tackle this problem, taking into account that sometimes, the more a problem can be sub-divided, the more balanced will be the execution times of the process \cite{Hill}.%\cite{Rezgui2013, Hill}. 
In \cite{Arbab2000} Arbab and Monfory propose a mechanism to create sub-\csps{} (whose union contains all the solutions of the original \csp) by splitting the domain of the variables. The coordination is achieved though communication between processes. The contribution of this work is explained in details in Section~\ref{sec:cooperation}. 

In \cite{Yasuhara2015} \etal{Yasuhara} propose a new search method called \textit{Multi-Objective Embarrassingly Parallel Search} (MO--EPS) to solve multi-objective optimization problems, based on: 
\begin{inparaenum}[i)]
	\item Embarrassingly Parallel Search (EPS), where the initial problem is split into a number of independent sub-problems, by partitioning the domain of decision variables \cite{Regin2014}; and %\cite{Rezgui2013, Regin2014}; and
	\item Multi-Objective optimization adding cuts (MO--AC), an algorithm that transforms the multi-objective optimization problem into a feasibility one, searches a feasible solution and then the search is continued adding constraints to the problem until either the problem becomes infeasible or the search space gets entirely explored \cite{Kotecha2010}.
\end{inparaenum}
Multi-objective optimization problems involve more than one objective function to be optimized simultaneously. Usually these problems do not have an unique optimal solution because there exist a trade-off between one objective function and the others. For that reason, in a multi-objective optimization problem, the concept of \textit{pareto optimal} points is used. A pareto optimal point is a solution that improving one or some objective function values, implies the deterioration of at least one of the other objective function. %A collection of pareto optimal points defines a pareto front.

Related to parallelizing the search process, we can find two main approaches. First, the {\it single walk} approach, in which all the processes try to follow the same path towards the solution, solving their corresponding part of the problem, with or without cooperation (communication). The other is known as {\it multi walk}, consisting of the execution of various independent processes to find the solution. Each process applies its own strategies (portfolio approach) or simply explores different places inside the search space. Although this approach may seem too trivial and not so smart, it is fair to say that it is in fashion due to the good obtained results using it \cite{Diaz}.

\etal{Kishimoto} present in \cite{Kishimoto2013} a comparison between \textit{Transposition-table Driven Scheduling} (TDS) and a parallel implementation of a best-first search strategy (Hash Distributed A$^*$), that uses the standard approach of \textit{work stealing} for partitioning the search space. This technique is based on maintaining a local work queue, (provided by a root process through hash-based distribution that assign an unique processor to each work) accessible to other process that "steal" work from it if they become unoccupied. Authors use MPI, the paradigm of \textit{Message Passing Interface} that allows parallelization, not only in distributed memory based architectures, but also in shared memory based architectures and mixed environments (clusters of multi-core machines) \cite{Grama2003a}. The same approach is used by Jinnai and Fukunaga in \cite{Jinnai} to evaluate \textit{Zobrist Hashing}, an efficient hash function designed for table games like Chess and Go, to mitigate communication overheads.

In \cite{Arbelaez2012} \etal{Arbelaez} present a study of the impact of space-partitioning techniques on the performance of parallel local search algorithms to tackle the \textit{k-medoids} clustering problem. Using a parallel local search method, this work aims to improve the scalability of the sequential algorithm, which is measured in terms of the quality of the solution within a given timeout. Two main techniques are presented for domain partitioning: first, {\it space-filling curves}, used to reduce any N-dimensional representation into a one-dimension space (this technique is also widely used in the nearest-neighbor-finding problem \cite{Chen2005}); and second, {\it k-Means} algorithm, one of the most popular clustering algorithms \cite{Berkhin2002}.

An interesting work is presented by \etal{Truchet} in \cite{Truchet02}, which is an estimation of the speed-up (a performance prediction of a parallel algorithm) through statistical analysis of its sequential algorithm is presented. Using this approach it is possible to have a rough idea of the resources needed to solve a given problem in parallel. In this work, authors study the parallel performances of \textit{Las Vegas} algorithms \cite{Babai1979} (randomized algorithms whose runtime might vary from one execution to another, even with the same input) under independent multi-walk scheme, and predict the performances of the parallel execution from the runtime distribution of their sequential runs. These predictions are compared to actual speed--ups obtained for a parallel implementation of the same algorithm and show that the prediction can be quite accurate.

The other important aspect in parallel computing is the inter-process communication, also called \textit{solver cooperation} and it is treated in the next section.

%A lot of studies have been published around this topic. A parallel solver for numerical \csps{} is presented in \cite{Ishii2014} showing good results scaling on a number of cores.
%We can find in \cite{Diaz2012} a survey of the different parallel programming models and available tools, emphasizing on their suitability for high-performance computing.

%Some other efforts have been allocated in the exploitation of the power of calculus provided by the massively parallel architecture of the Graphic Processing Unit (GPU). In \cite{Arbelaez} are presented  implementations of efficient (and very fast) constraint-based local search solvers using GPU.
%\nocite{GPU}