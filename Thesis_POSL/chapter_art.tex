\chapter{State of the art}
\label{chap:art}
\textit{This chapter presents an overview to the state of the art of \COPs{} and different approaches to tackle them. In Section~\ref{sec:combi} the definition of a \COP{} and its links with \CSPs{} (\csp) are introduced, where I concentrate the main efforts, and I give some examples. The basic techniques used to solve these problems are introduced: {\it \cp} (Section~\ref{sec:cp}) and {\it meta--heuristic methods} (Section~\ref{sec:meta}). I also present some advanced techniques like {\it hyper--heuristic methods} in Section~\ref{sec:hyper}, {\it hybridization} in Section~\ref{sec:hybrid}, {\it parallel computing} in Section~\ref{sec:parallel}, and {\it solvers cooperation} in Section~\ref{sec:cooperation}. Finally, before ending the chapter with a brief summary,  I present {\it parameter setting techniques} in Section~\ref{sec:tunning}.}
\vfill
\minitoc
\newpage

%This chapter presents an overview to the state of the art of \COPs{} and different approaches to tackle them. In Section~\ref{sec:combi} the definition of a \CSP{} (\csp), emphasizing in the concept of \CSPs, where we concentrate our main efforts. Constraint propagation techniques are deterministic methods to attack these kind of problems (presented in Section~\ref{sec:progagation}), but in some cases they are incapable to solve them (they are mostly used to reduce the problem's search space or to prove it unsatisfiable). For that reason, the model presented in this thesis is based on \textit{meta-heuristic} methods (Section~\ref{sec:meta}). The \textit{Hybridization} approach combines different techniques in the same solution strategy, so the progresses in this field are exposed in Section~\ref{sec:hybrid}.

%The evolution of computer architecture is leading us toward massively multi-core computers for tomorrow, composed of thousands of computing units. A parallel model to solve \csps{} is the core of this work, and its advances, as well as those obtained in the field of \textit{cooperation between solvers}, are presented in Sections~\ref{sec:parallel} and \ref{sec:cooperation} respectively. Finally, this chapter presents in Section~\ref{sec:tunning} an overview of the progresses in the field of \textit{parameter settings}.  

\section{Combinatorial Optimization}\label{sec:combi}
\input{art/combi}

%An \textit{Optimization Problem} consists in finding the best solution among all possible ones, subject or not, to a set of constraints, depending on whether it is a restricted or an unrestricted problem. The suitable values for the involved variables belong to a set called {\it domain}. When this domain contains only discrete values, we are facing a \COP, and its goal is to find the best possible solution satisfying a global criterion, named {\it objective function}. {\it Resource Allocations} \cite{Akplogan2011}, \textit{Task Scheduling} \cite{Sibbesen2008}, \textit{Master-keying} \cite{Espelage2000}, \textit{Traveling Salesman}, \textit{Knapsack Problem}, among others, are well-known examples of \cops{} \cite{Smith2005}.

\section{Constraint programming}\label{sec:cp}
\input{art/cp}

\section{Meta-heuristic methods}\label{sec:meta}
\input{art/meta}

\section{Hyper-heuristic Methods}\label{sec:hyper}
\input{art/hyper}

\section{Hybridization}\label{sec:hybrid}
\input{art/hybr}


\section{Parallel computing}
\label{sec:parallel}

\tet{INCLUIR}: {\it ParadisEO}, a framework to design parallel and distributed hybrid meta-heuristics showing very good results \cite{Cahon2004}, including a broad range of reusable features to easily design evolutionary algorithms and local search methods.

Parallel computing is a way to solve problems using some calculus resources at the same time. It is a powerful alternative to solve problems which would require too much time by using the traditional ways, i.e., sequential algorithms \cite{Grama2003}. That is why this field is in constant development and it is the topic where I put most of our effort. 

For a couple of years, all processors in modern machines are multi-core. Massively parallel architectures, previously expensive and so far reserved for super--computers, become now a trend available to a broad public through hardware like the Xeon Phi or GPU cards. The power delivered by massively parallel architectures allow us to treat faster these problems \cite{Borkar2007}. However this architectural evolution is a non-sense if algorithms do not evolve at the same time: the development and the implementation of algorithms should take this into account and tackling the problems with very different methods, changing the sequential reasoning of researchers in Computer Science \cite{Hill2008, Sanders2014}. We can find in \cite{Diaz2012} a survey of the different parallel programming models and available tools, emphasizing on their suitability for high-performance computing.

Falcou propose in \cite{Falcou2009} a programming model: \textit{Parallel Algorithmic Skeletons} (along with a C++ implementation called {\sc Quaff} to make parallel application development easier. Writing efficient code for parallel machines is less trivial, as it usually involves dealing with low-level APIs such as OpenMP, message-passing interfaces (MPI), among others. However, years of experience have shown that using those frameworks is difficult and error-prone. Usually many undesired behaviors (like deadlocks) make parallel software development very slow compared to the classic, sequential approach. In that sense, this model is a high-order pattern to hide all low-level, architecture or framework dependent code from the user, and provides a decent level of organization. {\sc Quaff} is a skeleton-based parallel programming library, which has demonstrated its efficiency and expressiveness solving some application from computer vision, that relies on C++ template meta-programming to reduce the overhead traditionally associated with object-oriented implementations of such libraries: the code generation is done at compilation time.

%Some results have been obtained on this field. 
The contribution in terms of hardware has been crucial, achieving powerful technologies to perform large--scale calculations. But the development of the techniques and algorithms to solve problems in parallel is also visible, focusing the main efforts in three fundamentals concepts: 
\begin{enumerate}%\begin{inparaenum}[i)]
    \item {\it Problem subdivision},
    \item {\it Scalability} and
    \item {\it Inter-process communication}.
\end{enumerate}%\end{inparaenum}

In a preliminary review of literature on parallel constraint solving \cite{Gent}, addressing the literature in constraints on exploitation of parallel systems for constraint solving, is starting first by looking at the justification for the multi-core architecture. It presents an analysis of some limiting factors on performance such as \textit{Amdahl}'s law, and then reviews recent literature on parallel constraint programming, grouping the paper in four areas: 
\begin{inparaenum}[i)]
	\item parallelizing the search process,  
	\item parallel and distributed arc-consistency, 
	\item multi-agent and cooperative search and
	\item combined parallel search and parallel consistency.
\end{inparaenum}

The issue of sub-dividing a given problem in some smaller sub-problems is sometimes not easy to address. Even when we can do it, the time needed by each process to solve its own part of the problem is rarely balanced. In \cite{Rezgui2013} are proposed some techniques to tackle this problem, taking into account that sometimes, the more can be sub-divided a problem, the more balanced will be the execution times of the process. In \cite{Kishimoto2013} is presented an comparison between Transposition-table Driven Scheduling (TDS) and a parallel implementation of a best-first search strategy (Hash Distributed A$^*$), that uses the standard approach of of \textit{Work Stealing} for partitioning the search space. This technique is based on maintaining a local work queue, (provided by a \textit{root process} through hash-based distribution that assign an unique processor to each work) accessible to other process that "steal" work from if they become unoccupied. The same approach is used in \cite{Jinnai} to evaluate \textit{Zobrist Hashing}, an efficient hash function designed for table games like chess and Go, to mitigate communication overheads.

In \cite{Arbelaez2012} is presented a study of the impact of space-partitioning techniques on the performance of parallel local search algorithms to tackle the \textit{k-medoids} clustering problem. Using a parallel local search, this work aims to improve the scalability of the sequential algorithm, which is measured in terms of the quality of the solution within the same time with respect to the sequential algorithm. Two main techniques are presented for domain partitioning: first, {\it space-filling curves}, used to reduce any N-dimensional representation into a one-dimension space (this technique is also widely used in the nearest-neighbor-finding problem \cite{Chen2005}); and second, {\it k-Means} algorithm, one of the most popular clustering algorithms \cite{Berkhin2002}.

In \cite{Arbab2000} is proposed a mechanism to create sub-\csps{} (whose union contains all the solutions of the original \csp) by splitting the domain of the variables though communication between processes. The contribution of this work is explained in details in Section~\ref{sec:cooperation}.

Related to the search process, we can find two main approaches. First, the {\it single walk} approach, in which all the processes try to follow the same path towards the solution, solving their corresponding part of the problem, with or without cooperation (communication). The other is known as {\it multi walk}, and it proposes the execution of various independent processes to find the solution. Each process applies its own strategies (portfolio approach) or simply explores different places inside the search space. Although this approach may seem too trivial and not so smart, it is fair to say that it is in fashion due to the good obtained results using it \cite{Diaz}.

\textit{Scalability} is the ability of a system to handle the increasing growth of workload. A system which has improved over time its performance after adding work resources, and it is capable of doing it proportionally is called {\it scalable}. The increase has not been only in terms of calculus resources, but also in the amount of sub-problems coming from the sub-division of the original problem. The more we can divide a problem into smaller sub-problems, the faster we can solve it \cite{Hill}. \textit{Adaptive Search} is a good example of local search method that can scale up to a larger number of cores, e.g., a few hundreds or even thousands \cite{Diaz}. For this algorithm, an implementation of a cooperative multi-walks strategy has been published in \cite{Munera}. In this framework, the processes are grouped in teams to achieve search intensification, which cooperate with others teams through a head node (process) to achieve search diversification. Using an adaptation of this method, Munera et al. propose a parallel solution strategy able to solve hard instances of \textit{Stable Marriage with Incomplete List and Ties Problem} quickly. In \cite{Munera2016} is presented a combination of this method with an \textit{Extremal Optimization} procedure: a nature-inspired general-purpose meta-heuristic \cite{Boettcher2000}. 

A lot of studies have been published around this topic. A parallel solver for numerical \csps{} is presented in \cite{Ishii2014} showing good results scaling on a number of cores. In \cite{Truchet02}, an estimation of the speed-up (a performance measure of a parallel algorithm) through statistical analysis of its sequential algorithm is presented. This is a very interesting result because it a way to have a rough idea of the resources needed to solve a given problem in parallel.

Another issue to treat is the interprocess communication. Many times a close collaboration between process is required, in order to achieve the solution. But the first inconvenient is the slowness of the communication process. Some work have achieved to identify what information is viable to share. One example is \cite{Hamadi2012} where an idea to include low-level reasoning components in the SAT problems resolution is proposed. This approach allow us to perform the clause-shearing, controlling the exchange between any pair of process.

In \cite{Munera} is presented a new paradigm that includes cooperation between processes, in order to improve the independent multi-walk approach. In that case, cooperative search methods add a communication mechanism to the independent walk strategy, to share or exchange information between solver instances during the search process. This proposed framework is oriented towards distributed architectures based on clusters of nodes, with the notion of {\it teams} running on nodes and controlling several search engines ({\it explorers}) running on cores, and the idea that all teams are distributed and thus have limited inter--node communication. The communication between teams ensures diversification, while the communication between explorers is needed for intensification. This framework is oriented towards distributed architectures based on clusters of nodes, where teams are mapped to nodes and explorers run on cores. This framework was developed using the {\it X10 programming language}, which is a novel language for parallel processing developed by IBM Research, giving more flexibility than traditional approaches, e.g., MPI communication package.

In \cite{Frank2003} have been presented an implementation of the meta-solver framework which coordinates the cooperative work of arbitrary pluggable constraint solvers. This approach intents to integrate arbitrary, new or pre--existing constraint solvers, to form a system capable of solving complex mixed--domain constraint problems. The existing increased cooperation overhead is reduced through problem-specific cooperative solving strategies.

In \cite{Hamadi2011} is proposed the first {\it Deterministic Parallel DPLL} (A complete,  backtracking-based search algorithm for deciding the satisfiability of propositional logic formulas in conjunctive normal form) engine. The experimental results show that their approach preserves the performance of the parallel portfolio approach while ensuring full reproducibility of the results. Parallel exploration of the search space, defines a controlled environment based on a total ordering of solvers interactions through synchronization barriers. To maximize efficiency, information exchange (conflict-clauses) and check for termination are performed on a regular basis. The frequency of these exchanges greatly influences the performance of the solver. The paper explores the trade off between frequent synchronizing which allows the fast integration of foreign conflict--clauses at the cost of more synchronizing steps, and infrequent synchronizing at the cost of delayed foreign conflict--clauses integration.

Considering the problem of parallelizing restarted back--track search, in \cite{Cire2011} was developed a simple technique for parallelizing restarted search deterministically and it demonstrates experimentally that it can achieve near--linear speed--ups in practice, when the number of processors is constant and the number of restarts grows to infinity. They propose the following: Each parallel search process has its own local copy of a scheduling class which assigns restarts and their respective fail--limits to processors. This scheduling class computes the next {\it Luby} restart fail--limit and adds it to the processor that has the lowest number of accumulated fails so far, following an {\it earliest--start--time--first strategy}. Like this, the schedule is filled and each process can infer which is the next fail--limit that it needs to run based on the processor it is running on -- without communication. Overhead is negligible in practice since the scheduling itself runs extremely fast compared to CP search, and communication is limited to informing the other processes when a solution has been found.

In \cite{Guo2010}, were explored the two well--known principles of diversification and intensification in portfolio--based parallel SAT solving. To study their trade--off, they define two roles for the computational units. Some of them classified as {\it Masters} perform an original search strategy, ensuring diversification. The remaining units, classified as {\it Slaves} are there to intensify their master's strategy. There are some important questions to be answered:
\begin{inparaenum}[i)]
	\item what information should be given to a slave in order to intensify a given search effort?, 
	\item how often, a subordinated unit has to receive such information? and 
	\item the question of finding the number of subordinated units and their connections with the search efforts? 
\end{inparaenum}
The results lead to an original intensification strategy which outperforms the best parallel SAT solver {\it ManySAT}, and solves some open SAT instances.

Multi-objective optimization problems involve more than one objective function to be optimized simultaneously. Usually these problems do not have an unique optimal solution because the exist a trade-off between one objective function and the others. For that reason, in a multi-objective optimization problem, the concept of Pareto optimal points is used. A Pareto optimal point is a solution that improving one objective function value, implies the deterioration of at least one of the other objective function. A collection of Pareto optimal points defines a Pareto front. In \cite{Yasuhara2015}, is proposed a new search method, called \textit{Multi-Objective Embarrassingly Parallel Search} (MO--EPS) to solve multi-objective optimization problems, based on: 
\begin{inparaenum}[i)]
	\item Embarrassingly Parallel Search (EPS): where the initial problem is split into a number of independent sub-problems, by partitioning the domain of decision variables \cite{Rezgui2013, Regin2014}; and
	\item Multi-Objective optimization adding cuts (MO--AC): an algorithm that transforms the multi-objective optimization problem into a feasibility one, searches a feasible solution and then the search is continued adding constraints to the problem until either the problem becomes infeasible or the search space gets entirely explored \cite{Kotecha2010}.
\end{inparaenum}

A component-based constraint solver in parallel is proposed in \cite{Zoeteweij}. In this work, a parallel solver coordinates autonomous instances of a sequential constraint solver, which is used as a software component. The component solvers achieve load balancing of tree search through a time-out mechanism. It is implemented a specific mode of solver cooperation that aims at reducing the turn-around time of constraint solving through parallelization of tree search. The main idea is to try to solve a \csp{} before a time-out. If it can not find a solution, the algorithm defines a set of disjoint sub-problems to be distributed among a set of solvers running in parallel. The goal of the time-out mechanism is to provide an implicit load balancing: when a solver is busy, and there are no subproblems available, another solver produces new sub-problems when its time-out elapses.


%COMENTAR
%Some other efforts have been allocated in the exploitation of the power of calculus provided by the massively parallel architecture of the Graphic Processing Unit (GPU). In \cite{Arbelaez} are presented  implementations of efficient (and very fast) constraint-based local search solvers using GPU.
%\textcolor{green}{[not finished yet]}
%\nocite{GPU}

\section{Solvers cooperation}
\label{sec:cooperation}

The interaction between solvers exchanging some information is called {\it solver cooperation} and it is very popular in this field due to their good results. Its main goal is to improve some kind of limitations or inefficiency imposed by the use of unique solver. In practice, each solver runs in a computation unit, i.e. thread or processor. The cooperation is performed through inter--process communication, by using different methods: \textit{signals}, asynchronous notifications between processes in order to notify an event occurrence; \textit{semaphore}, an abstract data type for controlling access, by multiple processes, to a common resource; \textit{shared memory}, a memory simultaneously accessible by multiple processes; \textit{message passing}, allowing multiple programs to communicate using messages; among others.

Kishimoto et al. present in \cite{Kishimoto2009} a parallelization of the an algorithm A$^*$ (Hash Distributed A$^*$) for \textit{optimal sequential planning} \cite{Schmegner2004}, exploiting distributed memory computers clusters, to extract significant speedups from the hardware. In classical planning solving, both the memory and the CPU requirements are main causes of performance bottlenecks, so parallel algorithms have the potential to provide required resources to solve changeling instances. In \cite{Kishimoto2013}, authors study scalable parallel best-first search algorithms, using MPI, a paradigm of \textit{Message Passing Interface} that allows parallelization, not only in distributed memory based architectures, but also in shared memory based architectures and mixed environments (clusters of multi-core machines) \cite{Grama2003a}.

In \cite{Pajot2003} is presented a paradigm that enables the user to properly separate computation strategies from the search phases in solver cooperations. The cooperation must be supervised by the user, through {\it cooperation strategy language}, which defines the solver interactions in order to find the desired result.

In \cite{Hamadi2012}, an idea to include low-level reasoning components in the SAT problems resolution is proposed, dynamically adjusting the size of shared clauses to reduce the possible blow up in communication. \cite{Pajot2003} presents a paradigm that enables the user to properly separate strategies combining solver applications in order to find the desired result, from the way the search space is explored. 

{\it Meta--S} is an implementation of a theoretical framework proposed in \cite{Frank2003}, which allows to tackle problems, through the cooperation of arbitrary domain--specific constraint solvers. {\it Meta--S} \cite{Frank2003} is a practical implementation and extension of a theoretical framework, which allows the user to attack problems requiring the cooperation of arbitrary domain--specific constraint solvers. Through its modular structure and its extensible strategy specification language it also serves as a test--bed for generic and problem--specific (meta--)solving strategies, which are employed to minimize the incurred cooperation overhead. Treating the employed solvers as black boxes, the meta--solver takes constraints from a global pool and propagates them to the individual solvers, which are in return requested to provide newly gained information (i.e., constraints) back to the meta--solver, through variable projections. The major advantage of this approach lies in the ability to integrate arbitrary, new or pre--existing constraint solvers, to form a system that is capable of solving complex mixed--domain constraint problems, at the price of increased cooperation overhead. This overhead can however be reduced through more intelligent and/or problem--specific cooperative solving strategies. {\sc Hyperion} \cite{Brownlee2014} is an already mentioned framework for meta-- and hyper--heuristics built with the principle of interoperability, generality by providing generic templates for a variety of local search and evolutionary computation algorithms; and efficiency, allowing rapid prototyping with the possibility of reusing source code.

Arbab and Monfory propose in \cite{Arbab2000} a technique to guide the search by splitting the domain of variables. A \textit{Master} process builds the network of variables and domain reduction functions, and sends this informations to the worker agents. They workers concentrate their efforts on only one sub-CSP and the \textit{Master} collects solutions. The main advantage is that by changing only the search agent, different kinds of search can be performed. The coordination process is managing using the {\sc Manifold} coordination language \cite{Arbab1995}.

%{\sc Manifold} is a strongly-typed, block-structured, event-driven language for managing events, dynamically changing interconnections among sets of independent, concurrent and cooperative processes. A {\sc Manifold} application consists of a number of processes running on a heterogeneous network. Processes in the same application may be written in different programming languages. {\sc Manifold} has been successfully used in a broad range of applications \cite{Arbab1995}.

\section{Parameter setting techniques}
\label{sec:tunning}

Most of these methods to tackle combinatorial problems, involve a number of parameters that govern their behavior, and they need to be well adjusted, and most of the times they depend on the nature of the specific problem, so they require a previous analysis to study their behavior \cite{Birattari2005}. That is way another branch of the investigation arises: {\it parameter tuning}. It is also known as a meta optimization problem, because the main goal is to find the best solution (parameter configuration) for a program, which will try to find the best solution for some problem as well. In order to measure the quality of some found parameter setting for a program (solver), one of these criteria are taken into consideration: the speed of the run or the quality of the found solution for the problem that it solves.

The are tow classes to classify these methods: 
\begin{enumerate}
\item \textit{Off-line tunning}: Also known just as parameter tuning, were parameters are computed before the run.
\item \textit{On-line tunning}: Also known as parameter control, were parameters are adjusted during the run, and
\end{enumerate}

\subsection{Off-line tunning}

The technique of parameter tuning or off-line tunning, is used to computed the best parameter configuration for an algorithm before the run (solving a given instance of a problem), to obtain the best performance. Most of algorithms are very sensible to their parameters. This is the case of Evolutionary Algorithms (EA), were some parameters define the behavior of the algorithm. In \cite{A.E.Eiben2012} is presented a study of methods to tune these algorithms.

In \cite{Riff2013} is presented \textit{EVOCA}, a tool which allows meta-heuristics designers to obtain good results searching a good parameter configuration with no too much effort, by using the tool during the iterative design process. Holger H. Hoos highlights in \cite{Hoos2012} the efficacy of the technique named {\it racing procedure}, that is based on choosing a set of model problems and adjusting the parameters through a certain number of solver runs, discarding configurations that show a behavior substantially worse than the best already obtained so far. 

{\sc ParamsILS} (version 2.3) is a tool for parameter optimization for parametrized algorithms, which uses powerful stochastic local search methods and it has been applied with success in many combinatorial problems in order to find the best parameter configuration \cite{Hutter2009}. It is an open source program written in {\it Ruby}, and the public source include some examples and a detailed and complete User Guide with a compact explanation about how to use it with a specific solver \cite{Hutter2008}.

{\sc Revac} is a method based on information theory to measure parameter relevance, that calibrates the parameters of EAs in a robust way. Instead of estimating the performance of an EA for different parameter values, the method estimates the expected performance when parameter values are chosen from a given probability density distribution $C$. The method iteratively refines the probability distribution $C$ over possible parameter sets, and starting with a uniform distribution $C_0$ over the initial parameter space $\mathcal{X}$, the method gives a higher and higher probability to regions of $\mathcal{X}$ that increase the expected performance of the target EA \cite{Nannen2007}. In \cite{Smit2010} is presented a case study demonstrating that using the {\sc Revac} the "world champion" EA (the winner of the CEC-2005 competition) can be improved with few effort.

Another technique was successfully used to tune automatically parameters for EAs, through a model based on a {\it case-based reasoning} system. It attempts to imitate the human behavior in solving problems: look in the memory how we have solved a similar problem \cite{Yeguas2014} .

\subsection{On-line tunning}

Although parameter tunning shows to be an effective way to adjust parameters to sensibles algorithms, in some problems the optimal parameter settings may be different for various phases of the search process. This is the main motivation to use on-line tuning techniques to find the best parameter setting, also called \textit{Parameter Control Techniques}. Parameter control techniques are further divided into 
\begin{inparaenum}[i)]
\item \textit{deterministic parameter control}, where the value of a strategy parameter is altered by some deterministic rule, ignoring any feedback; 
\item \textit{adaptive parameter control}, which continually update their parameters using feedback from the population or the search, and this feedback is used to determine the direction or magnitude of the parameter changes; and 
\item \textit{self-adaptive parameter control}, which assign different parameters to each individual, Here the parameters to be adapted are coded into the chromosomes that undergo mutation and recombination, but these parameters are coded into the chromosomes that undergo mutation and recombination
\end{inparaenum}\cite{Eiben1999}.

Differential Evolution (DE) algorithm has been demonstrated to be an efficient, effective and robust optimization method. However, its performance is very sensitive to the parameters setting, and this dependency changes from problem to problem. The selection of proper parameters for a particular optimization problem is a quite complicate subject, especially in the multi-objective optimization field. This is the reason why many researchers are motivated to develop techniques to set the parameters automatically.

Liu et al. propose in \cite{Liu2005} an adaptive approach which uses fuzzy logic controllers to guide the search parameters, with the novelty of changing the mutation control parameter and the crossover during the optimization process. A self-adaptive DE (SaDE) algorithm is proposed in \cite{Qin2009}, where both trial vector generation strategies and their associated control parameter values are gradually adjusted by learning from the way they have generated their previous promising solutions, eliminating this way the time-consuming exhaustive search for the most suitable parameter setting. This algorithm has been generalized to multi-objective realm, with objective-wise learning strategies (OW-MOSaDE) \cite{Huang2009}.

Drozdik et al. present in \cite{Drozdik} a study of various approaches to find out if one can find an inherently better one in terms of performance and whether the parameter control mechanisms can find favorable parameters in problems which can be successfully optimized only with a limited set of parameters. They focused in the most important parameters: 
\begin{inparaenum}[1)]
\item the \textit{scaling factor}, which controls the structure of new invidious; and
\item the \textit{crossover probability}.
\end{inparaenum}

{\sc Meta-GAs} \cite{Clune2005} is a genetic self-adapting algorithm, adjusting genetic operators of genetic algorithms. In this paper the authors propose an approach of moving towards a Genetic Algorithm that does not require a fixed and predefined parameter setting, because it evolves during the run.

\section{Summary and discussion}

In this chapter I have presented an overview of the different techniques to solve \CSPs{}. Special attention was given to the \textit{local-search meta-heuristics}, as well as \textit{parallel computing}, which are directly related to this investigation.

In contrast with tree-based methods (complete methods), \textit{Meta-heuristic methods} have shown good results solving large and complex \csps, where the search space is huge. They are algorithms applying different techniques to guide the search as direct as possible through the solution. The main contribution of this thesis is presented in Chapter~\ref{chap:posl}, where is proposed a framework to build local-search meta-heuristics combining small functions (\oms) through an operator-based language. \textit{Hybridization} is also an important point in this investigation due to their good results in solving \csps. With the proposed framework, many different solvers can be created using solvers templates (\ass), that can be instantiated with different \oms.

The era of multi/many-core computers, and the development of parallel algorithms have opened new ways to solve constraint problems. In this field, the solver cooperation has become a very popular technique. In general, the main goal of parallelism is to improve some limitations imposed by the use of unique solver. The present investigation attempts to show the importance and the success of this technique, by proposing a deep study of some parallel \comstrs{} in Chapter~\ref{chap:expe}.