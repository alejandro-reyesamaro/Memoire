\chapter{State of the art}
\label{chap:art}
\textit{This chapter presents an overview to the state of the art of \COPs{} and different approaches to tackle them. In Section~\ref{sec:combi} the definition of a \COP{} and its links with \CSPs{} (\csp) are introduced, where I concentrate the main efforts, and I give some examples. The basic techniques used to solve these problems are introduced: {\it \cp} (Section~\ref{sec:cp}) and {\it meta--heuristic methods} (Section~\ref{sec:meta}). I also present some advanced techniques like {\it hyper--heuristic methods} in Section~\ref{sec:hyper}, {\it hybridization} in Section~\ref{sec:hybrid}, {\it parallel computing} in Section~\ref{sec:parallel}, and {\it solvers cooperation} in Section~\ref{sec:cooperation}. Finally, before ending the chapter with a brief summary,  I present {\it parameter setting techniques} in Section~\ref{sec:tunning}.}
\vfill
\minitoc
\newpage

%This chapter presents an overview to the state of the art of \COPs{} and different approaches to tackle them. In Section~\ref{sec:combi} the definition of a \CSP{} (\csp), emphasizing in the concept of \CSPs, where we concentrate our main efforts. Constraint propagation techniques are deterministic methods to attack these kind of problems (presented in Section~\ref{sec:progagation}), but in some cases they are incapable to solve them (they are mostly used to reduce the problem's search space or to prove it unsatisfiable). For that reason, the model presented in this thesis is based on \textit{meta-heuristic} methods (Section~\ref{sec:meta}). The \textit{Hybridization} approach combines different techniques in the same solution strategy, so the progresses in this field are exposed in Section~\ref{sec:hybrid}.

%The evolution of computer architecture is leading us toward massively multi-core computers for tomorrow, composed of thousands of computing units. A parallel model to solve \csps{} is the core of this work, and its advances, as well as those obtained in the field of \textit{cooperation between solvers}, are presented in Sections~\ref{sec:parallel} and \ref{sec:cooperation} respectively. Finally, this chapter presents in Section~\ref{sec:tunning} an overview of the progresses in the field of \textit{parameter settings}.  

\section{Combinatorial Optimization}\label{sec:combi}
\input{art/combi}

%An \textit{Optimization Problem} consists in finding the best solution among all possible ones, subject or not, to a set of constraints, depending on whether it is a restricted or an unrestricted problem. The suitable values for the involved variables belong to a set called {\it domain}. When this domain contains only discrete values, we are facing a \COP, and its goal is to find the best possible solution satisfying a global criterion, named {\it objective function}. {\it Resource Allocations} \cite{Akplogan2011}, \textit{Task Scheduling} \cite{Sibbesen2008}, \textit{Master-keying} \cite{Espelage2000}, \textit{Traveling Salesman}, \textit{Knapsack Problem}, among others, are well-known examples of \cops{} \cite{Smith2005}.

\section{Constraint programming}\label{sec:cp}
\input{art/cp}

\section{Meta-heuristic methods}\label{sec:meta}
\input{art/meta}

\section{Hyper-heuristic Methods}\label{sec:hyper}
\input{art/hyper}

\section{Hybridization}\label{sec:hybrid}
\input{art/hybr}

\section{Parallel computing}\label{sec:parallel}
\input{art/par}

\section{Solvers cooperation}\label{sec:cooperation}
\input{art/cooper}

\section{Parameter setting techniques}
\label{sec:tunning}

Most of these methods to tackle combinatorial problems, involve a number of parameters that govern their behavior, and they need to be well adjusted, and most of the times they depend on the nature of the specific problem, so they require a previous analysis to study their behavior \cite{Birattari2005}. That is way another branch of the investigation arises: {\it parameter tuning}. It is also known as a meta optimization problem, because the main goal is to find the best solution (parameter configuration) for a program, which will try to find the best solution for some problem as well. In order to measure the quality of some found parameter setting for a program (solver), one of these criteria are taken into consideration: the speed of the run or the quality of the found solution for the problem that it solves.

The are tow classes to classify these methods: 
\begin{enumerate}
\item \textit{Off-line tunning}: Also known just as parameter tuning, were parameters are computed before the run.
\item \textit{On-line tunning}: Also known as parameter control, were parameters are adjusted during the run, and
\end{enumerate}

\subsection{Off-line tunning}

The technique of parameter tuning or off-line tunning, is used to computed the best parameter configuration for an algorithm before the run (solving a given instance of a problem), to obtain the best performance. Most of algorithms are very sensible to their parameters. This is the case of Evolutionary Algorithms (EA), were some parameters define the behavior of the algorithm. In \cite{A.E.Eiben2012} is presented a study of methods to tune these algorithms.

In \cite{Riff2013} is presented \textit{EVOCA}, a tool which allows meta-heuristics designers to obtain good results searching a good parameter configuration with no too much effort, by using the tool during the iterative design process. Holger H. Hoos highlights in \cite{Hoos2012} the efficacy of the technique named {\it racing procedure}, that is based on choosing a set of model problems and adjusting the parameters through a certain number of solver runs, discarding configurations that show a behavior substantially worse than the best already obtained so far. 

{\sc ParamsILS} (version 2.3) is a tool for parameter optimization for parametrized algorithms, which uses powerful stochastic local search methods and it has been applied with success in many combinatorial problems in order to find the best parameter configuration \cite{Hutter2009}. It is an open source program written in {\it Ruby}, and the public source include some examples and a detailed and complete User Guide with a compact explanation about how to use it with a specific solver \cite{Hutter2008}.

{\sc Revac} is a method based on information theory to measure parameter relevance, that calibrates the parameters of EAs in a robust way. Instead of estimating the performance of an EA for different parameter values, the method estimates the expected performance when parameter values are chosen from a given probability density distribution $C$. The method iteratively refines the probability distribution $C$ over possible parameter sets, and starting with a uniform distribution $C_0$ over the initial parameter space $\mathcal{X}$, the method gives a higher and higher probability to regions of $\mathcal{X}$ that increase the expected performance of the target EA \cite{Nannen2007}. In \cite{Smit2010} is presented a case study demonstrating that using the {\sc Revac} the "world champion" EA (the winner of the CEC-2005 competition) can be improved with few effort.

Another technique was successfully used to tune automatically parameters for EAs, through a model based on a {\it case-based reasoning} system. It attempts to imitate the human behavior in solving problems: look in the memory how we have solved a similar problem \cite{Yeguas2014} .

\subsection{On-line tunning}

Although parameter tunning shows to be an effective way to adjust parameters to sensibles algorithms, in some problems the optimal parameter settings may be different for various phases of the search process. This is the main motivation to use on-line tuning techniques to find the best parameter setting, also called \textit{Parameter Control Techniques}. Parameter control techniques are further divided into 
\begin{inparaenum}[i)]
\item \textit{deterministic parameter control}, where the value of a strategy parameter is altered by some deterministic rule, ignoring any feedback; 
\item \textit{adaptive parameter control}, which continually update their parameters using feedback from the population or the search, and this feedback is used to determine the direction or magnitude of the parameter changes; and 
\item \textit{self-adaptive parameter control}, which assign different parameters to each individual, Here the parameters to be adapted are coded into the chromosomes that undergo mutation and recombination, but these parameters are coded into the chromosomes that undergo mutation and recombination
\end{inparaenum}\cite{Eiben1999}.

Differential Evolution (DE) algorithm has been demonstrated to be an efficient, effective and robust optimization method. However, its performance is very sensitive to the parameters setting, and this dependency changes from problem to problem. The selection of proper parameters for a particular optimization problem is a quite complicate subject, especially in the multi-objective optimization field. This is the reason why many researchers are motivated to develop techniques to set the parameters automatically.

Liu et al. propose in \cite{Liu2005} an adaptive approach which uses fuzzy logic controllers to guide the search parameters, with the novelty of changing the mutation control parameter and the crossover during the optimization process. A self-adaptive DE (SaDE) algorithm is proposed in \cite{Qin2009}, where both trial vector generation strategies and their associated control parameter values are gradually adjusted by learning from the way they have generated their previous promising solutions, eliminating this way the time-consuming exhaustive search for the most suitable parameter setting. This algorithm has been generalized to multi-objective realm, with objective-wise learning strategies (OW-MOSaDE) \cite{Huang2009}.

Drozdik et al. present in \cite{Drozdik} a study of various approaches to find out if one can find an inherently better one in terms of performance and whether the parameter control mechanisms can find favorable parameters in problems which can be successfully optimized only with a limited set of parameters. They focused in the most important parameters: 
\begin{inparaenum}[1)]
\item the \textit{scaling factor}, which controls the structure of new invidious; and
\item the \textit{crossover probability}.
\end{inparaenum}

{\sc Meta-GAs} \cite{Clune2005} is a genetic self-adapting algorithm, adjusting genetic operators of genetic algorithms. In this paper the authors propose an approach of moving towards a Genetic Algorithm that does not require a fixed and predefined parameter setting, because it evolves during the run.

\section{Summary and discussion}

In this chapter I have presented an overview of the different techniques to solve \CSPs{}. Special attention was given to the \textit{local-search meta-heuristics}, as well as \textit{parallel computing}, which are directly related to this investigation.

In contrast with tree-based methods (complete methods), \textit{Meta-heuristic methods} have shown good results solving large and complex \csps, where the search space is huge. They are algorithms applying different techniques to guide the search as direct as possible through the solution. The main contribution of this thesis is presented in Chapter~\ref{chap:posl}, where is proposed a framework to build local-search meta-heuristics combining small functions (\oms) through an operator-based language. \textit{Hybridization} is also an important point in this investigation due to their good results in solving \csps. With the proposed framework, many different solvers can be created using solvers templates (\ass), that can be instantiated with different \oms.

The era of multi/many-core computers, and the development of parallel algorithms have opened new ways to solve constraint problems. In this field, the solver cooperation has become a very popular technique. In general, the main goal of parallelism is to improve some limitations imposed by the use of unique solver. The present investigation attempts to show the importance and the success of this technique, by proposing a deep study of some parallel \comstrs{} in Chapter~\ref{chap:expe}.